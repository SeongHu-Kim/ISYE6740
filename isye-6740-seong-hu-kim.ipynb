{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566},{"sourceId":671851,"sourceType":"datasetVersion","datasetId":338555},{"sourceId":1877714,"sourceType":"datasetVersion","datasetId":1118008},{"sourceId":9695632,"sourceType":"datasetVersion","datasetId":5928199},{"sourceId":9862609,"sourceType":"datasetVersion","datasetId":6034593}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pytorch Audio Emotion Classifier - Neel Patel","metadata":{}},{"cell_type":"markdown","source":"# 1. Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Go to !End of DataPreprocessing to run the cells before this","metadata":{}},{"cell_type":"code","source":"# Import packages!\n\nimport os\nimport shutil\nfrom zipfile import ZipFile\nimport time\nimport random\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy\nimport librosa\n\nfrom IPython.display import Audio\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchaudio\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\nimport torchaudio.transforms as T\nfrom sklearn.model_selection import train_test_split\n\nimport torch.multiprocessing as mp\n\nmp.set_start_method(\"spawn\", force=True)\n\n#Use GPU acceleration if possible\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\n   \nprint(f'Using {device}') \n\n# Set seeds for reproducibility\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)  # If using CUDA\n\nAUGMENTATIONS_LOADED = True","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:43.480388Z","iopub.execute_input":"2024-11-11T16:24:43.481232Z","iopub.status.idle":"2024-11-11T16:24:47.156667Z","shell.execute_reply.started":"2024-11-11T16:24:43.481183Z","shell.execute_reply":"2024-11-11T16:24:47.155670Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#Import file_path\n\n#Both sex\nRAVDESS_path ='/kaggle/input/speech-emotion-recognition-en/Ravdess/audio_speech_actors_01-24/'\n\n#Both Sex\nCrema_path = '/kaggle/input/speech-emotion-recognition-en/Crema/'\ncrema_metadata_df = pd.read_csv('/kaggle/input/crema-metadata-extra-information/VideoDemographics.csv')\n\n#Only male\nSAVEE_path = '/kaggle/input/speech-emotion-recognition-en/Savee/' \n\n#Only female\nTESS_path = '/kaggle/input/speech-emotion-recognition-en/Tess/'\n\n\nif os.path.exists('/kaggle/input/metadata-and-augmentations'):\n\n    augmented_dir = '/kaggle/input/metadata-and-augmentations'\n    \n    augmented_training_df = pd.read_csv(augmented_dir+'/augmented_training_df.csv')\n    testing_df = pd.read_csv(augmented_dir+'/testing_df.csv')\n\n    AUGMENTATIONS_LOADED = True\n    os.chdir(augmented_dir)\n    print('Successfully loaded previous augmentations')\n\nelse:\n    AUGMENTATIONS_LOADED = False\n    print('Go to !End of DataPreprocessing cell in table of contents to run the cells before this in order to get dataaugmentations')\n\n# AUGMENTATIONS_LOADED = False\n# os.chdir('/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:47.158303Z","iopub.execute_input":"2024-11-11T16:24:47.158773Z","iopub.status.idle":"2024-11-11T16:24:47.259356Z","shell.execute_reply.started":"2024-11-11T16:24:47.158739Z","shell.execute_reply":"2024-11-11T16:24:47.258374Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Successfully loaded previous augmentations\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#crema_metadata_df.loc[crema_metadata_df['Ethnicity'] == 'Hispanic']\ncrema_metadata_df.loc[crema_metadata_df['ActorID'] == 1013]","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:47.260729Z","iopub.execute_input":"2024-11-11T16:24:47.261399Z","iopub.status.idle":"2024-11-11T16:24:47.281374Z","shell.execute_reply.started":"2024-11-11T16:24:47.261352Z","shell.execute_reply":"2024-11-11T16:24:47.280510Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"    ActorID  Age     Sex       Race Ethnicity\n12     1013   22  Female  Caucasian  Hispanic","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ActorID</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>Race</th>\n      <th>Ethnicity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>1013</td>\n      <td>22</td>\n      <td>Female</td>\n      <td>Caucasian</td>\n      <td>Hispanic</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def RAVDESS_extractor(audio_dir):\n    data_list = []\n    columns = ['Filename','Filepath','Gender','Emotion','Emotional Intensity']\n    RAV_metadata_df = pd.DataFrame(columns = columns)\n    \n    # Map identifiers to their corresponding values\n    emotion_dict = {\n      \"01\": \"neutral\", \"02\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\",\n      \"05\": \"angry\", \"06\": \"fear\", \"07\": \"disgust\", \"08\": \"surprised\"\n    }\n    \n    intensity_dict = {\"01\": \"medium\", \"02\": \"high\"}\n    statement_dict = {\"01\": \"Kids are talking by the door\", \"02\": \"Dogs are sitting by the door\"}\n    \n    \n    data_list = []\n    for actor_folder in os.listdir(audio_dir):\n      actor_path = os.path.join(audio_dir, actor_folder)\n    \n      if os.path.isdir(actor_path):  # Check if it's a folder\n            for file in os.listdir(actor_path):\n                if file.endswith(\".wav\"):\n                    parts = file.split(\".\")[0].split(\"-\") #first split the .wav extension then the '-'\n    \n                    # Extract metadata from the filename\n                    modality = parts[0]  # Not used, as itâ€™s audio-only for now\n                    vocal_channel = \"speech\" if parts[1] == \"01\" else \"song\"\n                    emotion = emotion_dict[parts[2]]\n                    emotional_intensity = intensity_dict[parts[3]]\n                    statement = statement_dict[parts[4]]\n                    actor_id = int(parts[6])\n                    gender = \"male\" if actor_id % 2 != 0 else \"female\"\n                    file_path = os.path.join(actor_path, file)  # Full path to the file\n                    \n                    # Append to datalist (ignoring the repetition)\n                    data_list.append({\n                        'Filename': file,\n                        'Filepath':file_path,\n                        'Gender': gender,\n                        'Emotion': emotion,\n                        'Emotional Intensity': emotional_intensity\n                    })\n    \n    df_addon = pd.DataFrame(data_list)\n    RAV_metadata_df = pd.concat([RAV_metadata_df, df_addon], ignore_index=True)\n\n    return RAV_metadata_df\n\ndef CREMA_extractor(audio_dir,crema_metadata_df):\n    data_list = []\n    emotion_map_dict = {'SAD':'sad',\n                       'ANG':'angry',\n                       'DIS':'disgust',\n                       'FEA':'fear',\n                       'HAP':'happy',\n                       'NEU':'neutral'}\n    intensity_dict = {'LO':'low',\n                     'MD':'medium',\n                     'HI':'high',\n                     'XX':'unknown',\n                     'X':'unknown'}\n\n    columns = ['Filename','Filepath','Gender','Emotion','Emotional Intensity']\n    crema_organized_df = pd.DataFrame(columns = columns)\n    \n    for file in os.listdir(audio_dir):\n        parts = file.split('.')[0].split('_')\n\n        file_name = file\n        file_path = os.path.join(audio_dir,file)\n        actor_id = int(parts[0])\n\n        gender = crema_metadata_df.loc[crema_metadata_df['ActorID'] == actor_id]['Sex'].values[0].lower()\n        emotion = emotion_map_dict[parts[2]]\n\n        #debugging\n        #print(file_name)\n        intensity = intensity_dict[parts[3]]\n\n        data_list.append({'Filename': file_name,\n                         'Filepath':file_path,\n                         'Gender':gender,\n                         'Emotion':emotion,\n                         'Emotional Intensity':intensity})\n\n    df_addon = pd.DataFrame(data_list)\n    crema_organized_df = pd.concat([crema_organized_df,df_addon],ignore_index=True)\n    return crema_organized_df\n\ndef SAVEE_extractor(audio_dir):\n    columns = ['Filename','Filepath','Gender','Emotion','Emotional Intensity']\n    savee_metadata_df = pd.DataFrame(columns = columns)\n\n    data_list = []\n    \n    emotion_map_dict = {'sa':'sad',\n                       'a':'angry',\n                       'd':'disgust',\n                       'f':'fear',\n                       'h':'happy',\n                       'n':'neutral',\n                        'su':'surprised'}\n\n    for file in os.listdir(audio_dir):\n        parts = file.split('.')[0].split('_')\n\n        file_name = file\n        file_path = os.path.join(audio_dir,file)\n        gender = 'male'\n        \n        emotion_code = \"\".join([s for s in parts[1] if s.isalpha()])\n        emotion = emotion_map_dict[emotion_code]\n        intensity = 'unknown'\n\n        data_list.append({'Filename': file_name,\n                         'Filepath':file_path,\n                         'Gender':gender,\n                         'Emotion':emotion,\n                         'Emotional Intensity':intensity})\n\n    df_addon = pd.DataFrame(data_list)\n    savee_metadata_df = pd.concat([savee_metadata_df,df_addon],ignore_index=True)\n    return savee_metadata_df\n\ndef TESS_extractor(audio_dir):\n    columns = ['Filename','Filepath','Gender','Emotion','Emotional Intensity']\n    tess_metadata_df = pd.DataFrame(columns = columns)\n    \n    emotion_map_dict = {'sad':'sad',\n                       'angry':'angry',\n                       'disgust':'disgust',\n                       'fear':'fear',\n                       'happy':'happy',\n                       'neutral':'neutral',\n                       'ps':'surprised'}\n    data_list = []\n    \n    for folder in os.listdir(audio_dir):\n      folder_path = os.path.join(audio_dir, folder)\n    \n      if os.path.isdir(folder_path):  # Check if it's a folder\n            for file in os.listdir(folder_path):\n                if file.endswith(\".wav\"):\n                    file_name = file\n                    file_path = os.path.join(folder_path, file)\n                    \n                    parts = file.split('.')[0].split('_')\n                    emotion = emotion_map_dict[parts[2].lower()]\n                    intensity = 'unknown'\n                    gender = 'female'\n                    \n                    data_list.append({'Filename': file_name,\n                         'Filepath':file_path,\n                         'Gender':gender,\n                         'Emotion':emotion,\n                         'Emotional Intensity':intensity})\n\n    df_addon = pd.DataFrame(data_list)\n    tess_metadata_df = pd.concat([tess_metadata_df,df_addon],ignore_index=True)\n    return tess_metadata_df","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:47.283597Z","iopub.execute_input":"2024-11-11T16:24:47.283905Z","iopub.status.idle":"2024-11-11T16:24:47.310927Z","shell.execute_reply.started":"2024-11-11T16:24:47.283874Z","shell.execute_reply":"2024-11-11T16:24:47.309857Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Get all the datasets metadata dataframe\nravdess_metadata_df = RAVDESS_extractor(RAVDESS_path)\ncrema_organized_df = CREMA_extractor(Crema_path,crema_metadata_df)\nsavee_metadata_df = SAVEE_extractor(SAVEE_path)\ntess_metadata_df = TESS_extractor(TESS_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:47.312069Z","iopub.execute_input":"2024-11-11T16:24:47.312416Z","iopub.status.idle":"2024-11-11T16:24:51.153470Z","shell.execute_reply.started":"2024-11-11T16:24:47.312384Z","shell.execute_reply":"2024-11-11T16:24:51.152684Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"ravdess_metadata_df['Emotion'].unique()\ncrema_organized_df['Emotion'].unique()\nsavee_metadata_df['Emotion'].unique()\ntess_metadata_df['Emotion'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:51.154481Z","iopub.execute_input":"2024-11-11T16:24:51.154792Z","iopub.status.idle":"2024-11-11T16:24:51.166629Z","shell.execute_reply.started":"2024-11-11T16:24:51.154761Z","shell.execute_reply":"2024-11-11T16:24:51.165667Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array(['fear', 'angry', 'disgust', 'neutral', 'sad', 'surprised', 'happy'],\n      dtype=object)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Combine the metadata for all dataframes!\n\ncombined_metadata_df = pd.concat([ravdess_metadata_df,\n                                  crema_organized_df,\n                                  savee_metadata_df,\n                                  tess_metadata_df])\n\ncombined_metadata_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:51.167789Z","iopub.execute_input":"2024-11-11T16:24:51.168113Z","iopub.status.idle":"2024-11-11T16:24:51.185645Z","shell.execute_reply.started":"2024-11-11T16:24:51.168081Z","shell.execute_reply":"2024-11-11T16:24:51.184766Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                   Filename  \\\n0  03-01-08-01-01-01-02.wav   \n1  03-01-01-01-01-01-02.wav   \n2  03-01-07-02-01-02-02.wav   \n3  03-01-07-01-01-02-02.wav   \n4  03-01-01-01-02-01-02.wav   \n\n                                            Filepath  Gender    Emotion  \\\n0  /kaggle/input/speech-emotion-recognition-en/Ra...  female  surprised   \n1  /kaggle/input/speech-emotion-recognition-en/Ra...  female    neutral   \n2  /kaggle/input/speech-emotion-recognition-en/Ra...  female    disgust   \n3  /kaggle/input/speech-emotion-recognition-en/Ra...  female    disgust   \n4  /kaggle/input/speech-emotion-recognition-en/Ra...  female    neutral   \n\n  Emotional Intensity  \n0              medium  \n1              medium  \n2                high  \n3              medium  \n4              medium  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Filename</th>\n      <th>Filepath</th>\n      <th>Gender</th>\n      <th>Emotion</th>\n      <th>Emotional Intensity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>03-01-08-01-01-01-02.wav</td>\n      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n      <td>female</td>\n      <td>surprised</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>03-01-01-01-01-01-02.wav</td>\n      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n      <td>female</td>\n      <td>neutral</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>03-01-07-02-01-02-02.wav</td>\n      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n      <td>female</td>\n      <td>disgust</td>\n      <td>high</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>03-01-07-01-01-02-02.wav</td>\n      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n      <td>female</td>\n      <td>disgust</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>03-01-01-01-02-01-02.wav</td>\n      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n      <td>female</td>\n      <td>neutral</td>\n      <td>medium</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"#Exploratory Data Analysis\n\nsns.countplot(data = combined_metadata_df, x = 'Emotion')\ncombined_metadata_df['Emotion'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:51.187397Z","iopub.execute_input":"2024-11-11T16:24:51.188095Z","iopub.status.idle":"2024-11-11T16:24:51.510599Z","shell.execute_reply.started":"2024-11-11T16:24:51.188051Z","shell.execute_reply":"2024-11-11T16:24:51.509263Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Emotion\ndisgust      1923\nsad          1923\nfear         1923\nhappy        1923\nangry        1923\nneutral      1895\nsurprised     652\nName: count, dtype: int64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"**Notice how surprise category is low on numbers! We should perform data augmentation only on the training dataset to avoid data leakage**\n\nThe game plan is to:\n* 1: Augment data to the surprised category to make it balanced\n* 2: Split the data into training and testing datasets\n* 3: Augment the training dataset\n","metadata":{}},{"cell_type":"code","source":"#Surprised dataframe\n\n\nsurprised_df = combined_metadata_df.loc[combined_metadata_df['Emotion'] == 'surprised']\n\nif AUGMENTATIONS_LOADED == False:\n    #Write Augmentation Functions and Helper Functions:\n    augmented_dir = \"augmented_surprised_samples\"\n    \n    #Delete everyfile in augmented_dirs_folder\n    if os.path.exists(augmented_dir):\n        shutil.rmtree(augmented_dir) \n    os.makedirs(augmented_dir, exist_ok=True)\n    \n    \n    #set random seed to be 42\n    random.seed(42)\n\ndef add_white_noise(waveform, \n                    noise_level=np.random.uniform(low =  0.0001,high = 0.001)):\n    noise = torch.randn_like(waveform, device = device) * noise_level\n    return waveform + noise\n\ndef time_stretch(waveform, sample_rate, rate=None):\n    if rate is None:\n        rate = np.random.uniform(0.8, 1.2)\n    waveform_np = waveform.squeeze().cpu().numpy()  # Convert to NumPy\n    stretched = librosa.effects.time_stretch(waveform_np, rate=rate)\n    return torch.tensor(stretched, device = device).unsqueeze(0)  # Convert back to tensor with channel dimension\n\ndef pitch_scale(waveform, sample_rate, \n                n_steps=None):\n    if n_steps is None:\n        n_steps = np.random.uniform(low = -1, high = 1)\n    waveform_np = waveform.squeeze().cpu().numpy()  # Convert to NumPy\n    pitched = librosa.effects.pitch_shift(waveform_np, sr=sample_rate, n_steps=n_steps)\n    return torch.tensor(pitched, device = device).unsqueeze(0)  # Convert back to tensor with channel dimension\n\ndef polarity_inversion(waveform):\n    return -waveform\n\ndef apply_gain(waveform, gain_factor = np.random.uniform(low = 5, high = 30)):\n    gain = torchaudio.transforms.Vol(gain = gain_factor, gain_type = 'amplitude').to(device)\n    return gain(waveform)\n\ndef apply_augmentations(waveform, sample_rate):\n    # List of possible augmentations\n    augmentations = [\n        lambda x: add_white_noise(x),\n        lambda x: time_stretch(x, sample_rate),\n        lambda x: pitch_scale(x, sample_rate),\n        lambda x: polarity_inversion(x),\n        lambda x: apply_gain(x)\n    ]\n    \n    # Randomly choose one or more augmentations to apply\n    num_augmentations = 3\n    selected_augmentations = random.sample(augmentations, num_augmentations)\n    \n    for augment in selected_augmentations:\n        waveform = augment(waveform)\n    return waveform\n\n\n# Assuming you have a dataframe 'surprised_df' with original \"surprised\" audio samples\ndef create_augmented_samples_surprised(surprised_df, new_sample_rate = None):\n    augmented_samples = []\n    \n    for version in range(1,3):\n        # First round of augmentation (AugmentedV1)\n        for i, row in tqdm(surprised_df.iterrows(), total = len(surprised_df), desc=f\"Augmenting Version {version}\"):\n            file_path = row['Filepath']\n            file_name = row['Filename'].split('.')[0]\n            \n            augmented_file_name = f\"{file_name}_surprised_augmentedV{version}_{i}.wav\"\n            gender = row['Gender']\n            emotional_intensity = row['Emotional Intensity']\n            \n            \n            waveform, original_sr = torchaudio.load(file_path)\n            waveform = waveform.to(device)\n            \n            #print(file_name)\n\n            # Ensure waveform is 2D [1, num_samples] if it's mono\n            if waveform.ndim > 1:\n                waveform = waveform.mean(dim = 0).unsqueeze(0)\n\n            #step1= time.time()\n            # Resample if necessary\n            if original_sr != new_sample_rate and new_sample_rate is not None:\n                resampler = torchaudio.transforms.Resample(original_sr, \n                                                           new_sample_rate,\n                                                           lowpass_filter_width=16, \n                                                           resampling_method='sinc_interp_hann').to(device)\n                waveform = resampler(waveform)\n\n            #step2= time.time()\n            #print(f'Resample time was {step1-step2}')\n\n            \n            # Apply augmentations to create the first round of augmented samples\n            augmented_waveform = apply_augmentations(waveform, original_sr)\n\n            augmented_waveform = augmented_waveform.cpu()\n\n            # Save augmented sample to a new file\n            augmented_file_path = os.path.join(augmented_dir, augmented_file_name)\n            torchaudio.save(augmented_file_path, augmented_waveform, original_sr, channels_first = True)\n\n            # Append augmented sample details to the list\n            augmented_samples.append({\n                'Filename': augmented_file_name,\n                'Filepath': augmented_file_path,\n                'Gender': gender,\n                'Emotion': 'surprised',  # Keep the same emotion label\n                'Emotional Intensity': emotional_intensity,\n                'Augmentation_Type': f'AugmentedV{version}'  # Tag the augmentation type\n            })\n\n    # Convert to DataFrame for compatibility\n    augmented_df = pd.DataFrame(augmented_samples)\n    return augmented_df\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:51.512037Z","iopub.execute_input":"2024-11-11T16:24:51.512345Z","iopub.status.idle":"2024-11-11T16:24:51.537964Z","shell.execute_reply.started":"2024-11-11T16:24:51.512313Z","shell.execute_reply":"2024-11-11T16:24:51.536987Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#Save and run the augmentations \n#Augment only the surprised_df to create new samples:\n\nif AUGMENTATIONS_LOADED == False:\n    sample_rate = 24414 \n    augmented_surprised_df = create_augmented_samples_surprised(surprised_df, \n                                                                new_sample_rate = sample_rate) \n    \n    shutil.make_archive('augmented_surprised_samples', 'zip', augmented_dir)\n    \n    # Download the zip file (if using a Jupyter environment)\n    from IPython.display import FileLink\n    FileLink('augmented_surprised_samples.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:24:51.541973Z","iopub.execute_input":"2024-11-11T16:24:51.542274Z","iopub.status.idle":"2024-11-11T16:24:51.548007Z","shell.execute_reply.started":"2024-11-11T16:24:51.542241Z","shell.execute_reply":"2024-11-11T16:24:51.547067Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"#Combine the augmented surprised with the combined_metadata_df \n\nif AUGMENTATIONS_LOADED == False:\n    combined_metadata_df = pd.concat((combined_metadata_df,\n                                      augmented_surprised_df.drop('Augmentation_Type',axis = 1)))\n    \n    sns.countplot(data = combined_metadata_df, x = 'Emotion')\n    combined_metadata_df['Emotion'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:51.549249Z","iopub.execute_input":"2024-11-11T16:24:51.549646Z","iopub.status.idle":"2024-11-11T16:24:51.555849Z","shell.execute_reply.started":"2024-11-11T16:24:51.549586Z","shell.execute_reply":"2024-11-11T16:24:51.554905Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#Splitting Training and Testing Data\n\n#gameplan:\n#1. Take nonaugmented surprised samples and make those the testing samples\n#2. Take the other emotions and create testing and training samples \n\n# Take nonaugmented surprised data for testing data\n\n\n#Create function for creating training and testing indices for other emotions\ndef creating_training_testing_split(combined_metadata_df, test_size ,drop_emotion = True, **kwargs):\n\n    \"\"\"\n    **kwargs can be the following:\n    emotion_to_drop = emotion\n    random_state = random_state for train_test_split\n\n    \"\"\"\n    for key,value in kwargs.items():\n        if key == 'emotion_to_drop':\n            emotion_to_drop = value\n        if key == 'random_state':\n            random_state = value\n            \n    emotions_list = list(combined_metadata_df['Emotion'].unique())\n    \n    if drop_emotion == True:\n        emotions_list.remove(emotion_to_drop)\n\n    test_indices = {}\n    train_indices = {}\n\n    training_df = pd.DataFrame(columns = combined_metadata_df.columns)\n    testing_df = pd.DataFrame(columns = combined_metadata_df.columns)\n\n    for emotion in emotions_list:\n        emotion_df = combined_metadata_df[combined_metadata_df['Emotion'] == emotion]\n        \n        #test_size = 652\n\n        train_idxs, test_idxs= train_test_split(range(len(emotion_df)), \n                                                 test_size = test_size, \n                                                 random_state = SEED)\n        #Append to dictionary  \n        test_indices[emotion] =  test_idxs\n        train_indices[emotion] =  train_idxs\n\n        emotion_df_train = emotion_df.iloc[train_indices[emotion],:]\n        emotion_df_test = emotion_df.iloc[test_indices[emotion],:]\n\n        training_df = pd.concat((training_df,emotion_df_train),axis = 0)\n        testing_df = pd.concat((testing_df,emotion_df_test),axis = 0)\n\n    return training_df, testing_df, train_indices, test_indices","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:51.557027Z","iopub.execute_input":"2024-11-11T16:24:51.557310Z","iopub.status.idle":"2024-11-11T16:24:51.568158Z","shell.execute_reply.started":"2024-11-11T16:24:51.557279Z","shell.execute_reply":"2024-11-11T16:24:51.567374Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#Create the training splits by calling the function\n\nif AUGMENTATIONS_LOADED == False:\n    surprised_df = combined_metadata_df.loc[combined_metadata_df['Emotion'] == 'surprised']\n    \n    nonaugmented_surprised_df = []\n    augmented_surprised_df = []\n    \n    for i,row in surprised_df.iterrows():\n        \n        gender = row['Gender']\n        filename = row['Filename'] \n        filepath = row['Filepath']\n        emotional_int = row['Emotional Intensity']\n        emotion = row['Emotion']\n        \n        if 'surprised_augmented' not in row['Filename']:\n            \n            nonaugmented_surprised_df.append({\n                'Filename': filename,\n                'Filepath': filepath,\n                'Gender': gender,\n                'Emotion': emotion,\n                'Emotional Intensity': emotional_int\n            })\n        else:\n            augmented_surprised_df.append({\n                'Filename': filename,\n                'Filepath': filepath,\n                'Gender': gender,\n                'Emotion': emotion,\n                'Emotional Intensity': emotional_int\n            })\n            \n    nonaugmented_surprised_df = pd.DataFrame(nonaugmented_surprised_df)\n    augmented_surprised_df = pd.DataFrame(augmented_surprised_df)\n    testing_samples = nonaugmented_surprised_df.shape[0]\n    \n    \n    training_df,testing_df,_,_ = creating_training_testing_split(combined_metadata_df, \n                                    test_size = testing_samples ,\n                                    drop_emotion = True, \n                                    emotion_to_drop = 'surprised')\n    \n    #append the surprised to the training dataframes and testing dataframes\n    training_df = pd.concat((augmented_surprised_df,training_df),axis = 0)\n    testing_df = pd.concat((nonaugmented_surprised_df,testing_df),axis = 0)\n    \n    \n    training_df.shape\n    \n    sns.countplot(data = training_df, x = 'Emotion')\n    combined_metadata_df['Emotion'].value_counts()\n    plt.show()\n    \n    sns.countplot(data = testing_df, x = 'Emotion')\n    combined_metadata_df['Emotion'].value_counts()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:51.569272Z","iopub.execute_input":"2024-11-11T16:24:51.569565Z","iopub.status.idle":"2024-11-11T16:24:51.580477Z","shell.execute_reply.started":"2024-11-11T16:24:51.569534Z","shell.execute_reply":"2024-11-11T16:24:51.579680Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Augment the training samples excluding the surprised category (since those are already augmented)","metadata":{}},{"cell_type":"code","source":"#Augment the training data\n\nif AUGMENTATIONS_LOADED == False:\n    non_surprised_df = combined_metadata_df.loc[combined_metadata_df['Emotion'] != 'surprised']\n    \n    #Write Augmentation Functions and Helper Functions:\n    augmented_dir = \"augmented_training_samples\"\n    \n    #Delete everyfile in augmented_dirs_folder\n    if os.path.exists(augmented_dir):\n        shutil.rmtree(augmented_dir) \n    os.makedirs(augmented_dir, exist_ok=True)\n\ndef create_training_augmentations(non_surprised_df, new_sample_rate):\n    augmented_training_samples = []\n    # First round of augmentation training samples (no surprised) (AugmentedV1)\n    \n    for i, row in tqdm(non_surprised_df.iterrows(),total = len(non_surprised_df),desc=f\"Augmenting Version 1\"):\n        file_path = row['Filepath']\n        file_name = row['Filename'].split('.')[0]\n\n        augmented_file_name = f\"{file_name}_augmented_training_V1_{i}.wav\"\n        gender = row['Gender']\n        emotional_intensity = row['Emotional Intensity']\n        emotion = row['Emotion']\n\n\n        waveform, original_sr = torchaudio.load(file_path)\n        waveform = waveform.to(device)\n        #print(file_name)\n\n        # Ensure waveform is 2D [1, num_samples] if it's mono\n        if waveform.ndim > 1:\n            waveform = waveform.mean(dim = 0).unsqueeze(0)\n            \n         # Resample if necessary\n        if original_sr != new_sample_rate and new_sample_rate is not None:\n            resampler = torchaudio.transforms.Resample(original_sr, \n                                                       new_sample_rate,\n                                                       lowpass_filter_width=16, \n                                                       resampling_method='sinc_interp_hann').to(device)\n            waveform = resampler(waveform)\n            \n        # Apply augmentations to create the first round of augmented samples\n        augmented_waveform = apply_augmentations(waveform, original_sr)\n        augmented_waveform = augmented_waveform.cpu()\n\n        # Save augmented sample to a new file\n        augmented_file_path = os.path.join(augmented_dir, augmented_file_name)\n        torchaudio.save(augmented_file_path, augmented_waveform, original_sr, channels_first = True)\n\n        # Append augmented sample details to the list\n        augmented_training_samples.append({\n            'Filename': augmented_file_name,\n            'Filepath': augmented_file_path,\n            'Gender': gender,\n            'Emotion': emotion,  # Keep the same emotion label\n            'Emotional Intensity': emotional_intensity,  \n            'Augmentation_Type': f'AugmentedV'  # Tag the augmentation type\n        })\n\n    # Convert to DataFrame for compatibility\n    augmented_training_df = pd.DataFrame(augmented_training_samples)\n    return augmented_training_df\n\nif AUGMENTATIONS_LOADED == False:\n    augmented_training_df = create_training_augmentations(non_surprised_df,new_sample_rate = 24414) ","metadata":{"execution":{"iopub.status.busy":"2024-11-11T16:24:51.581754Z","iopub.execute_input":"2024-11-11T16:24:51.582057Z","iopub.status.idle":"2024-11-11T16:24:51.595937Z","shell.execute_reply.started":"2024-11-11T16:24:51.582028Z","shell.execute_reply":"2024-11-11T16:24:51.595030Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"if AUGMENTATIONS_LOADED == False:\n    # Compress the augmented directory\n    shutil.make_archive('augmented_training_samples', 'zip', augmented_dir)\n    \n    # Download the zip file (if using a Jupyter environment)\n    from IPython.display import FileLink\n    FileLink('augmented_training_samples.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:24:51.597342Z","iopub.execute_input":"2024-11-11T16:24:51.597764Z","iopub.status.idle":"2024-11-11T16:24:51.606163Z","shell.execute_reply.started":"2024-11-11T16:24:51.597722Z","shell.execute_reply":"2024-11-11T16:24:51.605250Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\nif AUGMENTATIONS_LOADED == False:\n    #Create the testing samples dataset for kaggle\n    testing_df.to_csv('testing_df.csv', index=False)\n    augmented_training_df.to_csv('augmented_training_df.csv', index = False)\n    print(FileLink('testing_df.csv'))\n    print(FileLink('augmented_training_df.csv'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:24:51.607319Z","iopub.execute_input":"2024-11-11T16:24:51.607609Z","iopub.status.idle":"2024-11-11T16:24:51.614159Z","shell.execute_reply.started":"2024-11-11T16:24:51.607579Z","shell.execute_reply":"2024-11-11T16:24:51.613388Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#Create a folder called Metadata and Augmentations \n\nif AUGMENTATIONS_LOADED == False:\n\n    # Step 1: Create a folder for \"Metadata and Augmentations\"\n    output_folder = \"Metadata_and_Augmentations\"\n    if os.path.exists(output_folder):\n        shutil.rmtree(output_folder)  # Remove the folder if it exists to start fresh\n    os.makedirs(output_folder, exist_ok=True)\n    \n    # Step 2: Copy the necessary files to the folder\n    shutil.copy('augmented_training_df.csv', output_folder)\n    shutil.copy('testing_df.csv', output_folder)\n    \n    # Step 3: Unzip the augmented training and surprised samples folders\n    with ZipFile('augmented_training_samples.zip', 'r') as zip_ref:\n        zip_ref.extractall(os.path.join(output_folder, 'augmented_training_samples'))\n    \n    with ZipFile('augmented_surprised_samples.zip', 'r') as zip_ref:\n        zip_ref.extractall(os.path.join(output_folder, 'augmented_surprised_samples'))\n    \n    # Step 4: Create a zip file containing the \"Metadata and Augmentations\" folder\n    output_zip_file = 'Metadata_and_Augmentations.zip'\n    shutil.make_archive('Metadata_and_Augmentations', 'zip', output_folder)\n    \n    # Step 5: Provide a download link (if using a Jupyter notebook environment)\n    from IPython.display import FileLink\n    FileLink(output_zip_file)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:24:51.615146Z","iopub.execute_input":"2024-11-11T16:24:51.615448Z","iopub.status.idle":"2024-11-11T16:24:51.622906Z","shell.execute_reply.started":"2024-11-11T16:24:51.615418Z","shell.execute_reply":"2024-11-11T16:24:51.622086Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"os.listdir()\nfrom IPython.display import FileLink\nFileLink('Metadata_and_Augmentations.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:24:51.624002Z","iopub.execute_input":"2024-11-11T16:24:51.624293Z","iopub.status.idle":"2024-11-11T16:24:51.639317Z","shell.execute_reply.started":"2024-11-11T16:24:51.624248Z","shell.execute_reply":"2024-11-11T16:24:51.638565Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"/kaggle/input/metadata-and-augmentations/Metadata_and_Augmentations.zip","text/html":"Path (<tt>Metadata_and_Augmentations.zip</tt>) doesn't exist. It may still be in the process of being generated, or you may have the incorrect path."},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"# !End of Data Preprocessing \n**(Hint Select this cell, go Run -> run before and it should run all cells before this current cell)**\n\n**If you have ran this notebook, you should get 3 new datasets**:\n* augmented-surprised-samples\n* augmented-training-samples\n* testing-samples-metadata-csv","metadata":{}},{"cell_type":"code","source":"\n\n# These datasets are the result of data preprocessing cells you ran before this cell. \n\n#We need to make sure the audio files are the same size when creating the dataloader:\n\n#Helper functions:\ndef get_max_audio_length(metadata_df):\n    \"\"\"Calculate the maximum length of audio samples in the dataset.\"\"\"\n    start = time.time()\n    max_length = 0\n    for file_path in metadata_df['Filepath']:\n        waveform, sample_rate = torchaudio.load(file_path)\n        num_samples = waveform.shape[1]  # Number of samples in the waveform\n        if num_samples > max_length:\n            max_length = num_samples\n    end = time.time()\n    print (f'Max_length_found: {max_length}. Took {start - end} seconds')\n    return max_length\n\ndef same_length_batch(batch):\n    # Collate function to handle variable-length sequences\n\n    # Extract waveforms, emotions, and genders from the batch\n    waveforms = [item['waveform'].squeeze(0) for item in batch]  # Remove channel dimension if present (it's all mono anyways!)\n    emotions = torch.stack([item['emotion'] for item in batch]) #example [0,1,2]\n    genders = torch.stack([item['gender'] for item in batch]) #same as above\n    intensity = torch.stack([item['intensity'] for item in batch])\n    sample_rate = torch.stack([item['sample rate'] for item in batch])\n\n    \n    # Pad all waveforms to the same length (of the longest in the batch) ****THIS MEANS WE WILL HAVE TO MAKE THE NN VARIABLE LENGTH DEPENDING ON THE BATCH!\n    waveforms_padded = pad_sequence(waveforms, batch_first=True) #put the batch dimension first!\n    \n    # Return padded waveforms and corresponding labels\n    return {'waveform': waveforms_padded, \n            'emotion': emotions, \n            'gender': genders,\n            'vocal channel': vocal_channels,\n            'intensity':intensity,\n            'statement': statement,\n            'sample rate':sample_rate}\n\n#Create the custom pytorch dataset\n\nclass Emotion_Classification_Dataset(Dataset):\n    def __init__(self, metadata_df, \n                 transformations=None, \n                 same_length_all = True, \n                 target_length = None,\n                 target_sr = None,\n                 augment_surprised=False,\n                 device = device):\n        \"\"\"\n        Args:\n            metadata_df (DataFrame): DataFrame containing file paths and labels.\n            target_length (int): Target length for all audio samples in number of samples.\n            transformations (callable, optional): Optional list of transformations to be applied on a sample (e.g., audio augmentation).\n            same_length_all (Boolean): If True, enforce same length for all audio samples.\n        \"\"\"\n        self.metadata_df = metadata_df\n        self.transformations = transformations\n        self.same_length_all = same_length_all #Boolean\n        self.target_length = target_length\n        self.augment_surprised = augment_surprised  # Boolean for augmenting \"surprised\" category\n        self.target_sr = target_sr\n        \n    def __len__(self):\n        return len(self.metadata_df)\n\n    def __getitem__(self, idx):\n\n        start_time = time.time()\n\n    \n        # Get file path and labels from the DataFrame\n        file_path = self.metadata_df.iloc[idx]['Filepath']\n        emotion_label = self.metadata_df.iloc[idx]['Emotion']\n        gender_label = self.metadata_df.iloc[idx]['Gender']\n        intensity = self.metadata_df.iloc[idx]['Emotional Intensity']\n\n        step1_time = time.time()\n        print(f\"Step 1 (Fetching metadata): {step1_time - start_time:.4f} seconds\")\n        \n        # Load audio file (torchaudio returns waveform and sample rate)\n        waveform, sample_rate = torchaudio.load(file_path)\n        \n        step2_time = time.time()\n        print(f\"Step 2 (Loading audio): {step2_time - step1_time:.4f} seconds\")\n\n        \n        waveform = waveform.to(device)\n        \n        #Resample the data\n        #waveform = self.resample_if_necessary(waveform, sample_rate) \n        \n        step3_time = time.time()\n        print(f\"Step 3 (Resampling): {step3_time - step2_time:.4f} seconds\")\n\n\n        #Pad the data to make the same length\n        if self.same_length_all and self.target_length is not None:\n            waveform = self.pad_or_trim_waveform(waveform, self.target_length)\n            \n        step4_time = time.time()\n        print(f\"Step 4 (Padding/Trimming): {step4_time - step3_time:.4f} seconds\")\n\n        #waveform.cpu()\n        waveform_feature_dict = {'original waveform':waveform}\n        \n        # Extract features based on transformations\n        if self.transformations:\n            #waveform = waveform.to(device)\n            for transformation in self.transformations:\n                waveform_feature = transformation(waveform)\n                waveform_feature_dict[transformation.__class__.__name__] = waveform_feature#.cpu()\n                \n        step5_time = time.time()\n        print(f\"Step 5 (Transformations): {step5_time - step4_time:.4f} seconds\")\n\n        # Convert labels to tensors or numerical values\n        emotion_mapping = {\n            \"neutral\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3,\n            \"fear\": 4, \"disgust\": 5, \"surprised\": 6\n        }\n        gender_mapping = {\"male\": 0, \"female\": 1}\n        intensity_mapping = {\"low\": 0, \"medium\": 1, \"high\": 2, \"unknown\": 3}\n\n        step6_time = time.time()\n        print(f\"Step 6 (Label conversion): {step6_time - step5_time:.4f} seconds\")\n\n\n        emotion_tensor = torch.tensor(emotion_mapping[emotion_label])\n        gender_tensor = torch.tensor(gender_mapping[gender_label])\n        intensity_tensor = torch.tensor(intensity_mapping[intensity])\n        # You can return the labels as part of a dictionary\n        sample = {'waveform_features': waveform_feature_dict, \n                  'emotion': emotion_tensor, \n                  'gender': gender_tensor,\n                  'intensity': intensity_tensor,\n                  'sample rate':torch.tensor(sample_rate)}\n\n        total_time = time.time()\n        print(f\"Total time for __getitem__: {total_time - start_time:.4f} seconds\")\n        \n        return sample\n    \n    def pad_or_trim_waveform(self, waveform, target_length):\n        \"\"\"Pad or trim waveform to a fixed target length in samples.\"\"\"\n        num_samples = waveform.shape[1]  # waveform shape is (channels, num_samples)\n\n        if num_samples < target_length:\n            # Pad if the waveform is shorter than target length\n            padding = target_length - num_samples\n            waveform = F.pad(waveform, (0, padding))#.to(device) #pad the left with 0 0s and pad the right with padding amount of 0s\n        elif num_samples > target_length:\n            # Trim if the waveform is longer than target length\n            waveform = waveform[:, :target_length]\n\n        return waveform\n    \n    # def resample_if_necessary(self, waveform, sr):\n    #     if sr != self.target_sr:\n    #         resampler = torchaudio.transforms.Resample(sr, self.target_sr)\n    #         waveform = resampler(waveform) \n            \n    #     return waveform\n\n    def resample_if_necessary(self, waveform, sr):\n        if sr != self.target_sr:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sr)#.to(device) # Move to GPU\n            waveform = resampler(waveform)\n        return waveform\n\n\n#not in the dataloader class btw....\ndef load_dataset(metadata_df,\n                 same_length_all = True,\n                 sample_rate = 24414,\n                 seconds_of_audio = 3,\n                transformations = None):\n\n    # def worker_init_fn(worker_id):\n    #     random.seed(SEED)\n    #     np.random.seed(SEED)\n    \n    if same_length_all:\n        \n        #max length of our audio dataset (without any augmentation is 314818) \n        #max_length = get_max_audio_length(combined_metadata_df)\n        #max_length = 314818 #TRUE MAX LENGTH\n        \n        max_length = sample_rate * seconds_of_audio\n        combined_dataset = Emotion_Classification_Dataset(metadata_df=metadata_df,\n                                                transformations = transformations,\n                                                same_length_all = True,\n                                                target_length = max_length,\n                                                target_sr = 24414)\n        # dataloader = DataLoader(combined_dataset, \n        #                         batch_size=16, \n        #                         shuffle=True,  \n        #                         num_workers = 1)#,persistent_workers=True)\n\n        dataloader = DataLoader(combined_dataset, \n                                batch_size=16, \n                                shuffle=True)\n        \n    else:\n        combined_dataset = Emotion_Classification_Dataset(metadata_df=metadata_df,\n                                                transformations = transformations,\n                                                same_length_all = False,\n                                                target_sr = 24414)\n        dataloader = DataLoader(combined_dataset, \n                                batch_size=16, \n                                shuffle=True, \n                                collate_fn=same_length_batch, \n                                num_workers = 8,\n                                persistent_workers=True,\n                                worker_init_fn=worker_init_fn)\n    return dataloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:24:51.642298Z","iopub.execute_input":"2024-11-11T16:24:51.642571Z","iopub.status.idle":"2024-11-11T16:24:51.670404Z","shell.execute_reply.started":"2024-11-11T16:24:51.642542Z","shell.execute_reply":"2024-11-11T16:24:51.669532Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"\nstart = time.time()\ntransformations = [T.MFCC(sample_rate = 24414,\n                         n_mfcc = 40,\n                         log_mels = True,\n                         melkwargs = {\"n_fft\": 1024, \"hop_length\": 512, \"n_mels\": 64}).to(device)]\n\ndataloader = load_dataset(metadata_df = augmented_training_df,\n                          same_length_all=True,\n                          seconds_of_audio = 3,\n                          transformations = transformations) \nend = time.time()\n\nprint(end - start)\n# Iterate over the DataLoader (Print one iteration of the batch!)\nfor batch in tqdm(dataloader):\n    waveforms = batch['waveform_features']\n    emotions = batch['emotion']\n    genders = batch['gender']\n    sample_rate = batch['sample rate']\n    intensity = batch['intensity']\n    print(waveforms['original waveform'].shape, emotions, genders,intensity,sample_rate)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:24:51.671444Z","iopub.execute_input":"2024-11-11T16:24:51.671770Z","iopub.status.idle":"2024-11-11T16:24:53.160098Z","shell.execute_reply.started":"2024-11-11T16:24:51.671739Z","shell.execute_reply":"2024-11-11T16:24:53.159148Z"}},"outputs":[{"name":"stdout","text":"0.28095579147338867\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/720 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Step 1 (Fetching metadata): 0.0005 seconds\nStep 2 (Loading audio): 0.0764 seconds\nStep 3 (Resampling): 0.0002 seconds\nStep 4 (Padding/Trimming): 0.0179 seconds\nStep 5 (Transformations): 0.7645 seconds\nStep 6 (Label conversion): 0.0001 seconds\nTotal time for __getitem__: 0.8597 seconds\nStep 1 (Fetching metadata): 0.0003 seconds\nStep 2 (Loading audio): 0.0245 seconds\nStep 3 (Resampling): 0.0002 seconds\nStep 4 (Padding/Trimming): 0.0000 seconds\nStep 5 (Transformations): 0.0007 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0259 seconds\nStep 1 (Fetching metadata): 0.0003 seconds\nStep 2 (Loading audio): 0.0164 seconds\nStep 3 (Resampling): 0.0001 seconds\nStep 4 (Padding/Trimming): 0.0001 seconds\nStep 5 (Transformations): 0.0005 seconds\nStep 6 (Label conversion): 0.0001 seconds\nTotal time for __getitem__: 0.0175 seconds\nStep 1 (Fetching metadata): 0.0002 seconds\nStep 2 (Loading audio): 0.0214 seconds\nStep 3 (Resampling): 0.0001 seconds\nStep 4 (Padding/Trimming): 0.0001 seconds\nStep 5 (Transformations): 0.0004 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0223 seconds\nStep 1 (Fetching metadata): 0.0002 seconds\nStep 2 (Loading audio): 0.0193 seconds\nStep 3 (Resampling): 0.0002 seconds\nStep 4 (Padding/Trimming): 0.0001 seconds\nStep 5 (Transformations): 0.0007 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0206 seconds\nStep 1 (Fetching metadata): 0.0002 seconds\nStep 2 (Loading audio): 0.0188 seconds\nStep 3 (Resampling): 0.0002 seconds\nStep 4 (Padding/Trimming): 0.0001 seconds\nStep 5 (Transformations): 0.0005 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0199 seconds\nStep 1 (Fetching metadata): 0.0002 seconds\nStep 2 (Loading audio): 0.0170 seconds\nStep 3 (Resampling): 0.0001 seconds\nStep 4 (Padding/Trimming): 0.0001 seconds\nStep 5 (Transformations): 0.0004 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0179 seconds\nStep 1 (Fetching metadata): 0.0002 seconds\nStep 2 (Loading audio): 0.0162 seconds\nStep 3 (Resampling): 0.0002 seconds\nStep 4 (Padding/Trimming): 0.0001 seconds\nStep 5 (Transformations): 0.0004 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0171 seconds\nStep 1 (Fetching metadata): 0.0002 seconds\nStep 2 (Loading audio): 0.0144 seconds\nStep 3 (Resampling): 0.0001 seconds\nStep 4 (Padding/Trimming): 0.0001 seconds\nStep 5 (Transformations): 0.0004 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0152 seconds\nStep 1 (Fetching metadata): 0.0002 seconds\nStep 2 (Loading audio): 0.0224 seconds\nStep 3 (Resampling): 0.0002 seconds\nStep 4 (Padding/Trimming): 0.0001 seconds\nStep 5 (Transformations): 0.0004 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0233 seconds\nStep 1 (Fetching metadata): 0.0002 seconds\nStep 2 (Loading audio): 0.0184 seconds\nStep 3 (Resampling): 0.0002 seconds\nStep 4 (Padding/Trimming): 0.0001 seconds\nStep 5 (Transformations): 0.0007 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0196 seconds\nStep 1 (Fetching metadata): 0.0002 seconds\nStep 2 (Loading audio): 0.0232 seconds\nStep 3 (Resampling): 0.0002 seconds\nStep 4 (Padding/Trimming): 0.0000 seconds\nStep 5 (Transformations): 0.0004 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0241 seconds\nStep 1 (Fetching metadata): 0.0002 seconds\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/720 [00:01<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Step 2 (Loading audio): 0.0183 seconds\nStep 3 (Resampling): 0.0006 seconds\nStep 4 (Padding/Trimming): 0.0002 seconds\nStep 5 (Transformations): 0.0006 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0200 seconds\nStep 1 (Fetching metadata): 0.0003 seconds\nStep 2 (Loading audio): 0.0188 seconds\nStep 3 (Resampling): 0.0002 seconds\nStep 4 (Padding/Trimming): 0.0001 seconds\nStep 5 (Transformations): 0.0004 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0198 seconds\nStep 1 (Fetching metadata): 0.0002 seconds\nStep 2 (Loading audio): 0.0190 seconds\nStep 3 (Resampling): 0.0002 seconds\nStep 4 (Padding/Trimming): 0.0000 seconds\nStep 5 (Transformations): 0.0005 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0199 seconds\nStep 1 (Fetching metadata): 0.0002 seconds\nStep 2 (Loading audio): 0.0184 seconds\nStep 3 (Resampling): 0.0002 seconds\nStep 4 (Padding/Trimming): 0.0001 seconds\nStep 5 (Transformations): 0.0004 seconds\nStep 6 (Label conversion): 0.0000 seconds\nTotal time for __getitem__: 0.0193 seconds\ntorch.Size([16, 1, 73242]) tensor([3, 4, 4, 1, 2, 5, 0, 2, 4, 4, 5, 3, 0, 2, 2, 0]) tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1]) tensor([3, 2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3]) tensor([16000, 48000, 16000, 24414, 24414, 16000, 24414, 16000, 16000, 16000,\n        24414, 16000, 16000, 16000, 48000, 16000])\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"#Play audio file\n#Audio(batch['waveform_features'][5].squeeze(), rate = 24414)\n\nbatch['waveform_features']['original waveform'].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:24:53.161267Z","iopub.execute_input":"2024-11-11T16:24:53.161565Z","iopub.status.idle":"2024-11-11T16:24:53.167807Z","shell.execute_reply.started":"2024-11-11T16:24:53.161526Z","shell.execute_reply":"2024-11-11T16:24:53.166809Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([16, 1, 73242])"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"# 1.5 Feature Extraction\n\n0.0 values in the features means NaN","metadata":{}},{"cell_type":"code","source":"# import addtional packages\nfrom scipy.signal import convolve\n\nstart = time.time()\n\n# Load in preprocessed dataframes\ntraining = pd.read_csv('/kaggle/input/metadata-and-augmentations/augmented_training_df.csv')\ntesting = pd.read_csv('/kaggle/input/metadata-and-augmentations/testing_df.csv')\n\n# Make dataframes to store extracted feature values\ntraining_features = {\n    'Pitch - Mean': [0] * len(training),\n    'Pitch - Coefficient of Variation': [0] * len(training),\n    'Pitch - 20th Percentile': [0] * len(training),\n    'Pitch - 50th Percentile': [0] * len(training),\n    'Pitch - 80th Percentile': [0] * len(training),\n    'Pitch - Range between 20th and 80th Percentile': [0] * len(training),\n    'Pitch - Mean Rising Part': [0] * len(training),\n    'Pitch - Standard Deviation Rising Part': [0] * len(training),\n    'Pitch - Mean Falling Part': [0] * len(training),\n    'Pitch - Standard Deviation Falling Part': [0] * len(training),\n    'Jitter - Mean': [0] * len(training),\n    'Jitter - Coefficient of Variation': [0] * len(training),\n    'MFCC - Mean MFCC1': [0] * len(training),\n    'MFCC - Mean MFCC2': [0] * len(training),\n    'MFCC - Mean MFCC3': [0] * len(training),\n    'MFCC - Mean MFCC4': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC1': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC2': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC3': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC4': [0] * len(training),\n    'MFCC - Mean MFCC1 Voiced Region': [0] * len(training),\n    'MFCC - Mean MFCC2 Voiced Region': [0] * len(training),\n    'MFCC - Mean MFCC3 Voiced Region': [0] * len(training),\n    'MFCC - Mean MFCC4 Voiced Region': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC1 Voiced Region': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC2 Voiced Region': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC3 Voiced Region': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC4 Voiced Region': [0] * len(training)\n}\n\ntraining_features = pd.DataFrame(training_features, dtype = float)\n\ntesting_features = {\n    'Pitch - Mean': [0] * len(training),\n    'Pitch - Coefficient of Variation': [0] * len(training),\n    'Pitch - 20th Percentile': [0] * len(training),\n    'Pitch - 50th Percentile': [0] * len(training),\n    'Pitch - 80th Percentile': [0] * len(training),\n    'Pitch - Range between 20th and 80th Percentile': [0] * len(training),\n    'Pitch - Mean Rising Part': [0] * len(training),\n    'Pitch - Standard Deviation Rising Part': [0] * len(training),\n    'Pitch - Mean Falling Part': [0] * len(training),\n    'Pitch - Standard Deviation Falling Part': [0] * len(training),\n    'Jitter - Mean': [0] * len(training),\n    'Jitter - Coefficient of Variation': [0] * len(training),\n    'MFCC - Mean MFCC1': [0] * len(training),\n    'MFCC - Mean MFCC2': [0] * len(training),\n    'MFCC - Mean MFCC3': [0] * len(training),\n    'MFCC - Mean MFCC4': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC1': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC2': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC3': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC4': [0] * len(training),\n    'MFCC - Mean MFCC1 Voiced Region': [0] * len(training),\n    'MFCC - Mean MFCC2 Voiced Region': [0] * len(training),\n    'MFCC - Mean MFCC3 Voiced Region': [0] * len(training),\n    'MFCC - Mean MFCC4 Voiced Region': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC1 Voiced Region': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC2 Voiced Region': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC3 Voiced Region': [0] * len(training),\n    'MFCC - Coefficient of Variation MFCC4 Voiced Region': [0] * len(training)\n}\n\ntesting_features = pd.DataFrame(testing_features, dtype = float)\n\n\n\n# Conduct feature extraction for each audio file in the training and in the testing dataset\n# for i in range(0, len(training)):\n# for i in range(0, 10):\nfor i in tqdm(range(0, len(training)), desc=\"Processing\", unit=\"item\"):\n    \n    # Load in the audio file\n    path = training[\"Filepath\"].iloc[i]\n    waveform, sample_rate = torchaudio.load(path)\n\n    \n\n    # Pitch extraction\n    # Convert the waveform to numpy (librosa works with numpy arrays)\n    waveform_np = waveform.numpy()\n\n    # Use librosa to extract the pitch\n    # By default, the window size is usually 2048 samples, and the hop length (the distance between consecutive windows) is typically 512 samples.\n    f0, voiced_flag, voiced_probs = librosa.pyin(waveform_np, fmin=librosa.note_to_hz('A0'), fmax=librosa.note_to_hz('C8'))\n\n    if any(voiced_flag[0]):\n        \n        # Convert the pitch (f0) to a semitone scale starting at 27.5 Hz (A0)\n        f0_semitones = 12 * np.log2(f0 / 27.5)\n        \n        # Remove NaN values (no pitch detected)\n        f0_semitones_no_nan = f0_semitones[voiced_flag]  # only consider voiced frames\n    \n        # Apply symmetric moving average filter (3 frames long)\n        # The window size is 3, so the filter is [1/3, 1/3, 1/3] for a symmetric average.\n        window = np.ones(3) / 3  # symmetric moving average filter\n        smoothed_f0_semitones_no_nan = convolve(f0_semitones_no_nan, window, mode='same')\n    \n        # Arithmetic Mean (AM) - pitch funtional #1\n        mean_smoothed_f0_semitones_no_nan = np.mean(smoothed_f0_semitones_no_nan)\n        training_features.loc[i, \"Pitch - Mean\"] = mean_smoothed_f0_semitones_no_nan\n    \n        # Coefficient of Variation (CV) - pitch functional #2\n        std_smoothed_f0_semitones_no_nan = np.std(smoothed_f0_semitones_no_nan)  # Standard deviation\n        cv_smoothed_f0_semitones_no_nan = (std_smoothed_f0_semitones_no_nan / mean_smoothed_f0_semitones_no_nan) * 100  # Coefficient of Variation in percentage\n        # RuntimeWarning: invalid value encountered in scalar divide at line 116\n        training_features.loc[i, \"Pitch - Coefficient of Variation\"] = cv_smoothed_f0_semitones_no_nan\n    \n        # Calculate the 20th, 50th, and 80th percentiles - pitch functional #3, 4, and 5\n        percentile_20_smoothed_f0_semitones_no_nan = np.percentile(smoothed_f0_semitones_no_nan, 20)\n        training_features.loc[i, \"Pitch - 20th Percentile\"] = percentile_20_smoothed_f0_semitones_no_nan\n        percentile_50_smoothed_f0_semitones_no_nan = np.percentile(smoothed_f0_semitones_no_nan, 50)\n        training_features.loc[i, \"Pitch - 50th Percentile\"] = percentile_50_smoothed_f0_semitones_no_nan\n        percentile_80_smoothed_f0_semitones_no_nan = np.percentile(smoothed_f0_semitones_no_nan, 80)\n        training_features.loc[i, \"Pitch - 80th Percentile\"] = percentile_80_smoothed_f0_semitones_no_nan\n    \n        # Calculate the range between the 20th and 80th percentiles - pitch functional #6\n        percentile_range_smoothed_f0_semitones_no_nan = percentile_80_smoothed_f0_semitones_no_nan - percentile_20_smoothed_f0_semitones_no_nan\n        training_features.loc[i, \"Pitch - Range between 20th and 80th Percentile\"] = percentile_range_smoothed_f0_semitones_no_nan\n    \n        # Calculate the slope of the signal (difference between consecutive values)\n        slopes = np.diff(smoothed_f0_semitones_no_nan)\n        \n        # Separate rising and falling slopes\n        rising_slopes = slopes[slopes > 0]    # Only positive slopes (rising parts)\n        falling_slopes = slopes[slopes < 0]   # Only negative slopes (falling parts)\n        \n        # Calculate mean and standard deviation for rising parts - pitch functional #7 and 8\n        mean_rising_slope_smoothed_f0_semitones_no_nan = np.mean(rising_slopes) if len(rising_slopes) > 0 else 0\n        training_features.loc[i, \"Pitch - Mean Rising Part\"] = mean_rising_slope_smoothed_f0_semitones_no_nan\n        std_rising_slope_smoothed_f0_semitones_no_nan = np.std(rising_slopes) if len(rising_slopes) > 0 else 0\n        training_features.loc[i, \"Pitch - Standard Deviation Rising Part\"] = std_rising_slope_smoothed_f0_semitones_no_nan\n        \n        # Calculate mean and standard deviation for falling parts - pitch functional #9 and 10\n        mean_falling_slope_smoothed_f0_semitones_no_nan = np.mean(falling_slopes) if len(falling_slopes) > 0 else 0\n        training_features.loc[i, \"Pitch - Mean Falling Part\"] = mean_falling_slope_smoothed_f0_semitones_no_nan\n        std_falling_slope_smoothed_f0_semitones_no_nan = np.std(falling_slopes) if len(falling_slopes) > 0 else 0\n        training_features.loc[i, \"Pitch - Standard Deviation Falling Part\"] = std_falling_slope_smoothed_f0_semitones_no_nan\n\n\n    \n        # Jitter extraction\n        # Remove unvoiced frames (f0 is NaN for unvoiced segments)\n        f0_jitter_no_nan = f0[voiced_flag]  # only consider voiced frames\n        f0_jitter_no_nan = np.nan_to_num(f0_jitter_no_nan, nan=0.0)  # Replace any remaining NaNs with 0 for safety\n        \n        # Calculate pitch periods (T0) in seconds\n        T0 = 1 / f0_jitter_no_nan\n        T0[T0 == np.inf] = 0  # Handle any inf values resulting from division by zero\n        \n        # Smooth T0 with a symmetric moving average filter (3 frames long)\n        window = np.ones(3) / 3  # 3-frame moving average filter\n        smoothed_T0 = convolve(T0, window, mode='same')\n        \n        # Convert smoothed_T0 to a PyTorch tensor for further processing\n        smoothed_T0_tensor = torch.tensor(smoothed_T0, dtype=torch.float32)\n        \n        # Calculate jitter as the absolute difference between consecutive T0 values\n        jitter_values = torch.abs(smoothed_T0_tensor[1:] - smoothed_T0_tensor[:-1])\n        \n        # Calculate arithmetic mean of jitter - jitter functional #1\n        mean_jitter = torch.mean(jitter_values)\n        training_features.loc[i, \"Jitter - Mean\"] = mean_jitter.item()\n        \n        # Calculate coefficient of variation of jitter - jitter functional #2\n        std_jitter = torch.std(jitter_values)\n        cv_jitter = (std_jitter / mean_jitter) * 100  # Coefficient of Variation as percentage\n        training_features.loc[i, \"Jitter - Coefficient of Variation\"] = cv_jitter.item()\n    \n\n\n\n    # Mel-Frequency Cepstral Coefficients (MFCC) 1â€“4 extraction\n    # Convert the waveform to numpy (librosa works with numpy arrays)\n    waveform_np = waveform.numpy()\n    \n    # Extract MFCCs using librosa\n    mfccs = librosa.feature.mfcc(y = waveform_np, sr = sample_rate, n_mfcc=4)  # Extract only the first 4 MFCCs\n    \n    # Convert the MFCCs to a PyTorch tensor for further processing\n    mfccs_tensor = torch.tensor(mfccs, dtype=torch.float32)\n    \n    # Calculate arithmetic mean for each MFCC - MFCC functional #1, 2, 3, and 4\n    mean_mfcc1 = torch.mean(mfccs_tensor[0, 0, :])\n    training_features.loc[i, \"MFCC - Mean MFCC1\"] = mean_mfcc1.item()\n    mean_mfcc2 = torch.mean(mfccs_tensor[0, 1, :])\n    training_features.loc[i, \"MFCC - Mean MFCC2\"] = mean_mfcc2.item()\n    mean_mfcc3 = torch.mean(mfccs_tensor[0, 2, :])\n    training_features.loc[i, \"MFCC - Mean MFCC3\"] = mean_mfcc3.item()\n    mean_mfcc4 = torch.mean(mfccs_tensor[0, 3, :])\n    training_features.loc[i, \"MFCC - Mean MFCC4\"] = mean_mfcc4.item()\n\n    # Calculate coefficient of variation for each MFCC - MFCC functional #5, 6, 7, and 8\n    std_mfcc1 = torch.std(mfccs_tensor[0, 0, :])\n    std_mfcc2 = torch.std(mfccs_tensor[0, 1, :])\n    std_mfcc3 = torch.std(mfccs_tensor[0, 2, :])\n    std_mfcc4 = torch.std(mfccs_tensor[0, 3, :])\n    \n    cv_mfcc1 = (std_mfcc1 / mean_mfcc1) * 100  # Coefficient of Variation in percentage\n    training_features.loc[i, \"MFCC - Coefficient of Variation MFCC1\"] = cv_mfcc1.item()\n    cv_mfcc2 = (std_mfcc2 / mean_mfcc2) * 100\n    training_features.loc[i, \"MFCC - Coefficient of Variation MFCC2\"] = cv_mfcc2.item()\n    cv_mfcc3 = (std_mfcc3 / mean_mfcc3) * 100\n    training_features.loc[i, \"MFCC - Coefficient of Variation MFCC3\"] = cv_mfcc3.item()\n    cv_mfcc4 = (std_mfcc4 / mean_mfcc4) * 100\n    training_features.loc[i, \"MFCC - Coefficient of Variation MFCC4\"] = cv_mfcc4.item()\n\n    if any(voiced_flag[0]):\n    \n        # Calculate arithmetic mean for each MFCC only in the voiced region - MFCC functional #9, 10, 11, and 12\n        mean_mfcc1_voiced = torch.mean(mfccs_tensor[0, 0, voiced_flag[0]])\n        training_features.loc[i, \"MFCC - Mean MFCC1 Voiced Region\"] = mean_mfcc1_voiced.item()\n        mean_mfcc2_voiced = torch.mean(mfccs_tensor[0, 1, voiced_flag[0]])\n        training_features.loc[i, \"MFCC - Mean MFCC2 Voiced Region\"] = mean_mfcc2_voiced.item()\n        mean_mfcc3_voiced = torch.mean(mfccs_tensor[0, 2, voiced_flag[0]])\n        training_features.loc[i, \"MFCC - Mean MFCC3 Voiced Region\"] = mean_mfcc3_voiced.item()\n        mean_mfcc4_voiced = torch.mean(mfccs_tensor[0, 3, voiced_flag[0]])\n        training_features.loc[i, \"MFCC - Mean MFCC4 Voiced Region\"] = mean_mfcc4_voiced.item()\n    \n        # Calculate coefficient of variation for each MFCC only in the voiced region - MFCC functional #13, 14, 15, and 16\n        std_mfcc1_voiced = torch.std(mfccs_tensor[0, 0, voiced_flag[0]])\n        std_mfcc2_voiced = torch.std(mfccs_tensor[0, 1, voiced_flag[0]])\n        std_mfcc3_voiced = torch.std(mfccs_tensor[0, 2, voiced_flag[0]])\n        std_mfcc4_voiced = torch.std(mfccs_tensor[0, 3, voiced_flag[0]])\n        \n        cv_mfcc1_voiced = (std_mfcc1_voiced / mean_mfcc1_voiced) * 100  # Coefficient of Variation in percentage\n        training_features.loc[i, \"MFCC - Coefficient of Variation MFCC1 Voiced Region\"] = cv_mfcc1_voiced.item()\n        cv_mfcc2_voiced = (std_mfcc2_voiced / mean_mfcc2_voiced) * 100\n        training_features.loc[i, \"MFCC - Coefficient of Variation MFCC2 Voiced Region\"] = cv_mfcc2_voiced.item()\n        cv_mfcc3_voiced = (std_mfcc3_voiced / mean_mfcc3_voiced) * 100\n        training_features.loc[i, \"MFCC - Coefficient of Variation MFCC3 Voiced Region\"] = cv_mfcc3_voiced.item()\n        cv_mfcc4_voiced = (std_mfcc4_voiced / mean_mfcc4_voiced) * 100\n        training_features.loc[i, \"MFCC - Coefficient of Variation MFCC4 Voiced Region\"] = cv_mfcc4_voiced.item()\n\n\n\n# for i in range(0, len(testing)):\n# for i in range(0, 10):\nfor i in tqdm(range(0, len(testing)), desc=\"Processing\", unit=\"item\"):\n    \n    # Load in the audio file\n    path = testing[\"Filepath\"].iloc[i]\n    waveform, sample_rate = torchaudio.load(path)\n\n    \n\n    # Pitch extraction\n    # Convert the waveform to numpy (librosa works with numpy arrays)\n    waveform_np = waveform.numpy()\n\n    # Use librosa to extract the pitch\n    # By default, the window size is usually 2048 samples, and the hop length (the distance between consecutive windows) is typically 512 samples.\n    f0, voiced_flag, voiced_probs = librosa.pyin(waveform_np, fmin=librosa.note_to_hz('A0'), fmax=librosa.note_to_hz('C8'))\n\n    if any(voiced_flag[0]):\n    \n        # Convert the pitch (f0) to a semitone scale starting at 27.5 Hz (A0)\n        f0_semitones = 12 * np.log2(f0 / 27.5)\n    \n        # Remove NaN values (no pitch detected)\n        f0_semitones_no_nan = f0_semitones[voiced_flag]  # only consider voiced frames\n    \n        # Apply symmetric moving average filter (3 frames long)\n        # The window size is 3, so the filter is [1/3, 1/3, 1/3] for a symmetric average.\n        window = np.ones(3) / 3  # symmetric moving average filter\n        smoothed_f0_semitones_no_nan = convolve(f0_semitones_no_nan, window, mode='same')\n    \n        # Arithmetic Mean (AM) - pitch funtional #1\n        mean_smoothed_f0_semitones_no_nan = np.mean(smoothed_f0_semitones_no_nan)\n        testing_features.loc[i, \"Pitch - Mean\"] = mean_smoothed_f0_semitones_no_nan\n    \n        # Coefficient of Variation (CV) - pitch functional #2\n        std_smoothed_f0_semitones_no_nan = np.std(smoothed_f0_semitones_no_nan)  # Standard deviation\n        cv_smoothed_f0_semitones_no_nan = (std_smoothed_f0_semitones_no_nan / mean_smoothed_f0_semitones_no_nan) * 100  # Coefficient of Variation in percentage\n        testing_features.loc[i, \"Pitch - Coefficient of Variation\"] = cv_smoothed_f0_semitones_no_nan\n    \n        # Calculate the 20th, 50th, and 80th percentiles - pitch functional #3, 4, and 5\n        percentile_20_smoothed_f0_semitones_no_nan = np.percentile(smoothed_f0_semitones_no_nan, 20)\n        testing_features.loc[i, \"Pitch - 20th Percentile\"] = percentile_20_smoothed_f0_semitones_no_nan\n        percentile_50_smoothed_f0_semitones_no_nan = np.percentile(smoothed_f0_semitones_no_nan, 50)\n        testing_features.loc[i, \"Pitch - 50th Percentile\"] = percentile_50_smoothed_f0_semitones_no_nan\n        percentile_80_smoothed_f0_semitones_no_nan = np.percentile(smoothed_f0_semitones_no_nan, 80)\n        testing_features.loc[i, \"Pitch - 80th Percentile\"] = percentile_80_smoothed_f0_semitones_no_nan\n    \n        # Calculate the range between the 20th and 80th percentiles - pitch functional #6\n        percentile_range_smoothed_f0_semitones_no_nan = percentile_80_smoothed_f0_semitones_no_nan - percentile_20_smoothed_f0_semitones_no_nan\n        testing_features.loc[i, \"Pitch - Range between 20th and 80th Percentile\"] = percentile_range_smoothed_f0_semitones_no_nan\n    \n        # Calculate the slope of the signal (difference between consecutive values)\n        slopes = np.diff(smoothed_f0_semitones_no_nan)\n        \n        # Separate rising and falling slopes\n        rising_slopes = slopes[slopes > 0]    # Only positive slopes (rising parts)\n        falling_slopes = slopes[slopes < 0]   # Only negative slopes (falling parts)\n        \n        # Calculate mean and standard deviation for rising parts - pitch functional #7 and 8\n        mean_rising_slope_smoothed_f0_semitones_no_nan = np.mean(rising_slopes) if len(rising_slopes) > 0 else 0\n        testing_features.loc[i, \"Pitch - Mean Rising Part\"] = mean_rising_slope_smoothed_f0_semitones_no_nan\n        std_rising_slope_smoothed_f0_semitones_no_nan = np.std(rising_slopes) if len(rising_slopes) > 0 else 0\n        testing_features.loc[i, \"Pitch - Standard Deviation Rising Part\"] = std_rising_slope_smoothed_f0_semitones_no_nan\n        \n        # Calculate mean and standard deviation for falling parts - pitch functional #9 and 10\n        mean_falling_slope_smoothed_f0_semitones_no_nan = np.mean(falling_slopes) if len(falling_slopes) > 0 else 0\n        testing_features.loc[i, \"Pitch - Mean Falling Part\"] = mean_falling_slope_smoothed_f0_semitones_no_nan\n        std_falling_slope_smoothed_f0_semitones_no_nan = np.std(falling_slopes) if len(falling_slopes) > 0 else 0\n        testing_features.loc[i, \"Pitch - Standard Deviation Falling Part\"] = std_falling_slope_smoothed_f0_semitones_no_nan\n    \n    \n        \n        # Jitter extraction\n        # Remove unvoiced frames (f0 is NaN for unvoiced segments)\n        f0_jitter_no_nan = f0[voiced_flag]  # only consider voiced frames\n        f0_jitter_no_nan = np.nan_to_num(f0_jitter_no_nan, nan=0.0)  # Replace any remaining NaNs with 0 for safety\n        \n        # Calculate pitch periods (T0) in seconds\n        T0 = 1 / f0_jitter_no_nan\n        T0[T0 == np.inf] = 0  # Handle any inf values resulting from division by zero\n        \n        # Smooth T0 with a symmetric moving average filter (3 frames long)\n        window = np.ones(3) / 3  # 3-frame moving average filter\n        smoothed_T0 = convolve(T0, window, mode='same')\n        \n        # Convert smoothed_T0 to a PyTorch tensor for further processing\n        smoothed_T0_tensor = torch.tensor(smoothed_T0, dtype=torch.float32)\n        \n        # Calculate jitter as the absolute difference between consecutive T0 values\n        jitter_values = torch.abs(smoothed_T0_tensor[1:] - smoothed_T0_tensor[:-1])\n        \n        # Calculate arithmetic mean of jitter - jitter functional #1\n        mean_jitter = torch.mean(jitter_values)\n        testing_features.loc[i, \"Jitter - Mean\"] = mean_jitter.item()\n        \n        # Calculate coefficient of variation of jitter - jitter functional #2\n        std_jitter = torch.std(jitter_values)\n        cv_jitter = (std_jitter / mean_jitter) * 100  # Coefficient of Variation as percentage\n        testing_features.loc[i, \"Jitter - Coefficient of Variation\"] = cv_jitter.item()\n    \n\n\n\n    # Mel-Frequency Cepstral Coefficients (MFCC) 1â€“4 extraction\n    # Convert the waveform to numpy (librosa works with numpy arrays)\n    waveform_np = waveform.numpy()\n    \n    # Extract MFCCs using librosa\n    mfccs = librosa.feature.mfcc(y = waveform_np, sr = sample_rate, n_mfcc=4)  # Extract only the first 4 MFCCs\n    \n    # Convert the MFCCs to a PyTorch tensor for further processing\n    mfccs_tensor = torch.tensor(mfccs, dtype=torch.float32)\n    \n    # Calculate arithmetic mean for each MFCC - MFCC functional #1, 2, 3, and 4\n    mean_mfcc1 = torch.mean(mfccs_tensor[0, 0, :])\n    testing_features.loc[i, \"MFCC - Mean MFCC1\"] = mean_mfcc1.item()\n    mean_mfcc2 = torch.mean(mfccs_tensor[0, 1, :])\n    testing_features.loc[i, \"MFCC - Mean MFCC2\"] = mean_mfcc2.item()\n    mean_mfcc3 = torch.mean(mfccs_tensor[0, 2, :])\n    testing_features.loc[i, \"MFCC - Mean MFCC3\"] = mean_mfcc3.item()\n    mean_mfcc4 = torch.mean(mfccs_tensor[0, 3, :])\n    testing_features.loc[i, \"MFCC - Mean MFCC4\"] = mean_mfcc4.item()\n\n    # Calculate coefficient of variation for each MFCC - MFCC functional #5, 6, 7, and 8\n    std_mfcc1 = torch.std(mfccs_tensor[0, 0, :])\n    std_mfcc2 = torch.std(mfccs_tensor[0, 1, :])\n    std_mfcc3 = torch.std(mfccs_tensor[0, 2, :])\n    std_mfcc4 = torch.std(mfccs_tensor[0, 3, :])\n    \n    cv_mfcc1 = (std_mfcc1 / mean_mfcc1) * 100  # Coefficient of Variation in percentage\n    testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC1\"] = cv_mfcc1.item()\n    cv_mfcc2 = (std_mfcc2 / mean_mfcc2) * 100\n    testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC2\"] = cv_mfcc2.item()\n    cv_mfcc3 = (std_mfcc3 / mean_mfcc3) * 100\n    testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC3\"] = cv_mfcc3.item()\n    cv_mfcc4 = (std_mfcc4 / mean_mfcc4) * 100\n    testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC4\"] = cv_mfcc4.item()\n\n    if any(voiced_flag[0]):\n    \n        # Calculate arithmetic mean for each MFCC only in the voiced region - MFCC functional #9, 10, 11, and 12\n        mean_mfcc1_voiced = torch.mean(mfccs_tensor[0, 0, voiced_flag[0]])\n        testing_features.loc[i, \"MFCC - Mean MFCC1 Voiced Region\"] = mean_mfcc1_voiced.item()\n        mean_mfcc2_voiced = torch.mean(mfccs_tensor[0, 1, voiced_flag[0]])\n        testing_features.loc[i, \"MFCC - Mean MFCC2 Voiced Region\"] = mean_mfcc2_voiced.item()\n        mean_mfcc3_voiced = torch.mean(mfccs_tensor[0, 2, voiced_flag[0]])\n        testing_features.loc[i, \"MFCC - Mean MFCC3 Voiced Region\"] = mean_mfcc3_voiced.item()\n        mean_mfcc4_voiced = torch.mean(mfccs_tensor[0, 3, voiced_flag[0]])\n        testing_features.loc[i, \"MFCC - Mean MFCC4 Voiced Region\"] = mean_mfcc4_voiced.item()\n    \n        # Calculate coefficient of variation for each MFCC only in the voiced region - MFCC functional #13, 14, 15, and 16\n        std_mfcc1_voiced = torch.std(mfccs_tensor[0, 0, voiced_flag[0]])\n        std_mfcc2_voiced = torch.std(mfccs_tensor[0, 1, voiced_flag[0]])\n        std_mfcc3_voiced = torch.std(mfccs_tensor[0, 2, voiced_flag[0]])\n        std_mfcc4_voiced = torch.std(mfccs_tensor[0, 3, voiced_flag[0]])\n        \n        cv_mfcc1_voiced = (std_mfcc1_voiced / mean_mfcc1_voiced) * 100  # Coefficient of Variation in percentage\n        testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC1 Voiced Region\"] = cv_mfcc1_voiced.item()\n        cv_mfcc2_voiced = (std_mfcc2_voiced / mean_mfcc2_voiced) * 100\n        testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC2 Voiced Region\"] = cv_mfcc2_voiced.item()\n        cv_mfcc3_voiced = (std_mfcc3_voiced / mean_mfcc3_voiced) * 100\n        testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC3 Voiced Region\"] = cv_mfcc3_voiced.item()\n        cv_mfcc4_voiced = (std_mfcc4_voiced / mean_mfcc4_voiced) * 100\n        testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC4 Voiced Region\"] = cv_mfcc4_voiced.item()\n\n\n\n# Concatenate features to the original dataframes\ntraining_with_Features = pd.concat([training, training_features], axis=1)\ntesting_with_Features = pd.concat([testing, testing_features], axis=1)\n\n# Save the updated dataframes as .csv files\ntraining_with_Features.to_csv('/kaggle/working/training_with_Features.csv', index=False)\ntesting_with_Features.to_csv('/kaggle/working/testing_with_Features.csv', index=False)\n\nend = time.time()\n\nprint(end - start)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:33:36.137077Z","iopub.execute_input":"2024-11-11T16:33:36.137464Z","iopub.status.idle":"2024-11-12T00:27:45.980661Z","shell.execute_reply.started":"2024-11-11T16:33:36.137428Z","shell.execute_reply":"2024-11-12T00:27:45.979508Z"}},"outputs":[{"name":"stderr","text":"Processing:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3828/11510 [2:06:04<3:22:31,  1.58s/item]/tmp/ipykernel_30/173316285.py:117: RuntimeWarning: invalid value encountered in scalar divide\n  cv_smoothed_f0_semitones_no_nan = (std_smoothed_f0_semitones_no_nan / mean_smoothed_f0_semitones_no_nan) * 100  # Coefficient of Variation in percentage\nProcessing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11510/11510 [5:38:17<00:00,  1.76s/item] \nProcessing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4564/4564 [2:15:51<00:00,  1.79s/item]  \n","output_type":"stream"},{"name":"stdout","text":"28449.5660572052\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# 2. Model Building","metadata":{}},{"cell_type":"code","source":"#mode (CNN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:26:18.919785Z","iopub.execute_input":"2024-11-11T16:26:18.920096Z","iopub.status.idle":"2024-11-11T16:26:18.924053Z","shell.execute_reply.started":"2024-11-11T16:26:18.920063Z","shell.execute_reply":"2024-11-11T16:26:18.923211Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"start = time.time()\npath = '/kaggle/input/metadata-and-augmentations/augmented_training_samples/03-01-01-01-01-01-01_augmented_training_V1_322.wav'\ntorchaudio.load(path)\nend = time.time()\n\nprint(end - start)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T16:26:18.925182Z","iopub.execute_input":"2024-11-11T16:26:18.925455Z","iopub.status.idle":"2024-11-11T16:26:18.959815Z","shell.execute_reply.started":"2024-11-11T16:26:18.925424Z","shell.execute_reply":"2024-11-11T16:26:18.958973Z"}},"outputs":[{"name":"stdout","text":"0.025290727615356445\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}