{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33eafa69",
   "metadata": {
    "papermill": {
     "duration": 0.010526,
     "end_time": "2024-11-11T06:31:33.205177",
     "exception": false,
     "start_time": "2024-11-11T06:31:33.194651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pytorch Audio Emotion Classifier - Neel Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8992b761",
   "metadata": {
    "papermill": {
     "duration": 0.010711,
     "end_time": "2024-11-11T06:31:33.225949",
     "exception": false,
     "start_time": "2024-11-11T06:31:33.215238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00124e3",
   "metadata": {
    "papermill": {
     "duration": 0.009447,
     "end_time": "2024-11-11T06:31:33.245231",
     "exception": false,
     "start_time": "2024-11-11T06:31:33.235784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Go to !End of DataPreprocessing to run the cells before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef38074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:33.267473Z",
     "iopub.status.busy": "2024-11-11T06:31:33.266755Z",
     "iopub.status.idle": "2024-11-11T06:31:39.365216Z",
     "shell.execute_reply": "2024-11-11T06:31:39.364168Z"
    },
    "papermill": {
     "duration": 6.113198,
     "end_time": "2024-11-11T06:31:39.368313",
     "exception": false,
     "start_time": "2024-11-11T06:31:33.255115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# Import packages!\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "import librosa\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchaudio\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "#Use GPU acceleration if possible\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "   \n",
    "print(f'Using {device}') \n",
    "\n",
    "# Set seeds for reproducibility\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)  # If using CUDA\n",
    "\n",
    "AUGMENTATIONS_LOADED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a4d445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:39.390107Z",
     "iopub.status.busy": "2024-11-11T06:31:39.389212Z",
     "iopub.status.idle": "2024-11-11T06:31:39.478826Z",
     "shell.execute_reply": "2024-11-11T06:31:39.477761Z"
    },
    "papermill": {
     "duration": 0.102334,
     "end_time": "2024-11-11T06:31:39.480796",
     "exception": false,
     "start_time": "2024-11-11T06:31:39.378462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded previous augmentations\n"
     ]
    }
   ],
   "source": [
    "#Import file_path\n",
    "\n",
    "#Both sex\n",
    "RAVDESS_path ='/kaggle/input/speech-emotion-recognition-en/Ravdess/audio_speech_actors_01-24/'\n",
    "\n",
    "#Both Sex\n",
    "Crema_path = '/kaggle/input/speech-emotion-recognition-en/Crema/'\n",
    "crema_metadata_df = pd.read_csv('/kaggle/input/crema-metadata-extra-information/VideoDemographics.csv')\n",
    "\n",
    "#Only male\n",
    "SAVEE_path = '/kaggle/input/speech-emotion-recognition-en/Savee/' \n",
    "\n",
    "#Only female\n",
    "TESS_path = '/kaggle/input/speech-emotion-recognition-en/Tess/'\n",
    "\n",
    "\n",
    "if os.path.exists('/kaggle/input/metadata-and-augmentations'):\n",
    "\n",
    "    augmented_dir = '/kaggle/input/metadata-and-augmentations'\n",
    "    \n",
    "    augmented_training_df = pd.read_csv(augmented_dir+'/augmented_training_df.csv')\n",
    "    testing_df = pd.read_csv(augmented_dir+'/testing_df.csv')\n",
    "\n",
    "    AUGMENTATIONS_LOADED = True\n",
    "    os.chdir(augmented_dir)\n",
    "    print('Successfully loaded previous augmentations')\n",
    "\n",
    "else:\n",
    "    AUGMENTATIONS_LOADED = False\n",
    "    print('Go to !End of DataPreprocessing cell in table of contents to run the cells before this in order to get dataaugmentations')\n",
    "\n",
    "# AUGMENTATIONS_LOADED = False\n",
    "# os.chdir('/kaggle/working/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9938af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:39.502135Z",
     "iopub.status.busy": "2024-11-11T06:31:39.501810Z",
     "iopub.status.idle": "2024-11-11T06:31:39.520680Z",
     "shell.execute_reply": "2024-11-11T06:31:39.519802Z"
    },
    "papermill": {
     "duration": 0.031671,
     "end_time": "2024-11-11T06:31:39.522726",
     "exception": false,
     "start_time": "2024-11-11T06:31:39.491055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ActorID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "      <th>Ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1013</td>\n",
       "      <td>22</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ActorID  Age     Sex       Race Ethnicity\n",
       "12     1013   22  Female  Caucasian  Hispanic"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#crema_metadata_df.loc[crema_metadata_df['Ethnicity'] == 'Hispanic']\n",
    "crema_metadata_df.loc[crema_metadata_df['ActorID'] == 1013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf7ff3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:39.544488Z",
     "iopub.status.busy": "2024-11-11T06:31:39.544215Z",
     "iopub.status.idle": "2024-11-11T06:31:39.570496Z",
     "shell.execute_reply": "2024-11-11T06:31:39.569610Z"
    },
    "papermill": {
     "duration": 0.039374,
     "end_time": "2024-11-11T06:31:39.572404",
     "exception": false,
     "start_time": "2024-11-11T06:31:39.533030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RAVDESS_extractor(audio_dir):\n",
    "    data_list = []\n",
    "    columns = ['Filename','Filepath','Gender','Emotion','Emotional Intensity']\n",
    "    RAV_metadata_df = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    # Map identifiers to their corresponding values\n",
    "    emotion_dict = {\n",
    "      \"01\": \"neutral\", \"02\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\",\n",
    "      \"05\": \"angry\", \"06\": \"fear\", \"07\": \"disgust\", \"08\": \"surprised\"\n",
    "    }\n",
    "    \n",
    "    intensity_dict = {\"01\": \"medium\", \"02\": \"high\"}\n",
    "    statement_dict = {\"01\": \"Kids are talking by the door\", \"02\": \"Dogs are sitting by the door\"}\n",
    "    \n",
    "    \n",
    "    data_list = []\n",
    "    for actor_folder in os.listdir(audio_dir):\n",
    "      actor_path = os.path.join(audio_dir, actor_folder)\n",
    "    \n",
    "      if os.path.isdir(actor_path):  # Check if it's a folder\n",
    "            for file in os.listdir(actor_path):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    parts = file.split(\".\")[0].split(\"-\") #first split the .wav extension then the '-'\n",
    "    \n",
    "                    # Extract metadata from the filename\n",
    "                    modality = parts[0]  # Not used, as itâ€™s audio-only for now\n",
    "                    vocal_channel = \"speech\" if parts[1] == \"01\" else \"song\"\n",
    "                    emotion = emotion_dict[parts[2]]\n",
    "                    emotional_intensity = intensity_dict[parts[3]]\n",
    "                    statement = statement_dict[parts[4]]\n",
    "                    actor_id = int(parts[6])\n",
    "                    gender = \"male\" if actor_id % 2 != 0 else \"female\"\n",
    "                    file_path = os.path.join(actor_path, file)  # Full path to the file\n",
    "                    \n",
    "                    # Append to datalist (ignoring the repetition)\n",
    "                    data_list.append({\n",
    "                        'Filename': file,\n",
    "                        'Filepath':file_path,\n",
    "                        'Gender': gender,\n",
    "                        'Emotion': emotion,\n",
    "                        'Emotional Intensity': emotional_intensity\n",
    "                    })\n",
    "    \n",
    "    df_addon = pd.DataFrame(data_list)\n",
    "    RAV_metadata_df = pd.concat([RAV_metadata_df, df_addon], ignore_index=True)\n",
    "\n",
    "    return RAV_metadata_df\n",
    "\n",
    "def CREMA_extractor(audio_dir,crema_metadata_df):\n",
    "    data_list = []\n",
    "    emotion_map_dict = {'SAD':'sad',\n",
    "                       'ANG':'angry',\n",
    "                       'DIS':'disgust',\n",
    "                       'FEA':'fear',\n",
    "                       'HAP':'happy',\n",
    "                       'NEU':'neutral'}\n",
    "    intensity_dict = {'LO':'low',\n",
    "                     'MD':'medium',\n",
    "                     'HI':'high',\n",
    "                     'XX':'unknown',\n",
    "                     'X':'unknown'}\n",
    "\n",
    "    columns = ['Filename','Filepath','Gender','Emotion','Emotional Intensity']\n",
    "    crema_organized_df = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    for file in os.listdir(audio_dir):\n",
    "        parts = file.split('.')[0].split('_')\n",
    "\n",
    "        file_name = file\n",
    "        file_path = os.path.join(audio_dir,file)\n",
    "        actor_id = int(parts[0])\n",
    "\n",
    "        gender = crema_metadata_df.loc[crema_metadata_df['ActorID'] == actor_id]['Sex'].values[0].lower()\n",
    "        emotion = emotion_map_dict[parts[2]]\n",
    "\n",
    "        #debugging\n",
    "        #print(file_name)\n",
    "        intensity = intensity_dict[parts[3]]\n",
    "\n",
    "        data_list.append({'Filename': file_name,\n",
    "                         'Filepath':file_path,\n",
    "                         'Gender':gender,\n",
    "                         'Emotion':emotion,\n",
    "                         'Emotional Intensity':intensity})\n",
    "\n",
    "    df_addon = pd.DataFrame(data_list)\n",
    "    crema_organized_df = pd.concat([crema_organized_df,df_addon],ignore_index=True)\n",
    "    return crema_organized_df\n",
    "\n",
    "def SAVEE_extractor(audio_dir):\n",
    "    columns = ['Filename','Filepath','Gender','Emotion','Emotional Intensity']\n",
    "    savee_metadata_df = pd.DataFrame(columns = columns)\n",
    "\n",
    "    data_list = []\n",
    "    \n",
    "    emotion_map_dict = {'sa':'sad',\n",
    "                       'a':'angry',\n",
    "                       'd':'disgust',\n",
    "                       'f':'fear',\n",
    "                       'h':'happy',\n",
    "                       'n':'neutral',\n",
    "                        'su':'surprised'}\n",
    "\n",
    "    for file in os.listdir(audio_dir):\n",
    "        parts = file.split('.')[0].split('_')\n",
    "\n",
    "        file_name = file\n",
    "        file_path = os.path.join(audio_dir,file)\n",
    "        gender = 'male'\n",
    "        \n",
    "        emotion_code = \"\".join([s for s in parts[1] if s.isalpha()])\n",
    "        emotion = emotion_map_dict[emotion_code]\n",
    "        intensity = 'unknown'\n",
    "\n",
    "        data_list.append({'Filename': file_name,\n",
    "                         'Filepath':file_path,\n",
    "                         'Gender':gender,\n",
    "                         'Emotion':emotion,\n",
    "                         'Emotional Intensity':intensity})\n",
    "\n",
    "    df_addon = pd.DataFrame(data_list)\n",
    "    savee_metadata_df = pd.concat([savee_metadata_df,df_addon],ignore_index=True)\n",
    "    return savee_metadata_df\n",
    "\n",
    "def TESS_extractor(audio_dir):\n",
    "    columns = ['Filename','Filepath','Gender','Emotion','Emotional Intensity']\n",
    "    tess_metadata_df = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    emotion_map_dict = {'sad':'sad',\n",
    "                       'angry':'angry',\n",
    "                       'disgust':'disgust',\n",
    "                       'fear':'fear',\n",
    "                       'happy':'happy',\n",
    "                       'neutral':'neutral',\n",
    "                       'ps':'surprised'}\n",
    "    data_list = []\n",
    "    \n",
    "    for folder in os.listdir(audio_dir):\n",
    "      folder_path = os.path.join(audio_dir, folder)\n",
    "    \n",
    "      if os.path.isdir(folder_path):  # Check if it's a folder\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_name = file\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    \n",
    "                    parts = file.split('.')[0].split('_')\n",
    "                    emotion = emotion_map_dict[parts[2].lower()]\n",
    "                    intensity = 'unknown'\n",
    "                    gender = 'female'\n",
    "                    \n",
    "                    data_list.append({'Filename': file_name,\n",
    "                         'Filepath':file_path,\n",
    "                         'Gender':gender,\n",
    "                         'Emotion':emotion,\n",
    "                         'Emotional Intensity':intensity})\n",
    "\n",
    "    df_addon = pd.DataFrame(data_list)\n",
    "    tess_metadata_df = pd.concat([tess_metadata_df,df_addon],ignore_index=True)\n",
    "    return tess_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1191009a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:39.593569Z",
     "iopub.status.busy": "2024-11-11T06:31:39.593263Z",
     "iopub.status.idle": "2024-11-11T06:31:43.466291Z",
     "shell.execute_reply": "2024-11-11T06:31:43.465360Z"
    },
    "papermill": {
     "duration": 3.886175,
     "end_time": "2024-11-11T06:31:43.468680",
     "exception": false,
     "start_time": "2024-11-11T06:31:39.582505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all the datasets metadata dataframe\n",
    "ravdess_metadata_df = RAVDESS_extractor(RAVDESS_path)\n",
    "crema_organized_df = CREMA_extractor(Crema_path,crema_metadata_df)\n",
    "savee_metadata_df = SAVEE_extractor(SAVEE_path)\n",
    "tess_metadata_df = TESS_extractor(TESS_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a62b6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:43.490711Z",
     "iopub.status.busy": "2024-11-11T06:31:43.490348Z",
     "iopub.status.idle": "2024-11-11T06:31:43.501671Z",
     "shell.execute_reply": "2024-11-11T06:31:43.500829Z"
    },
    "papermill": {
     "duration": 0.024345,
     "end_time": "2024-11-11T06:31:43.503512",
     "exception": false,
     "start_time": "2024-11-11T06:31:43.479167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fear', 'angry', 'disgust', 'neutral', 'sad', 'surprised', 'happy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ravdess_metadata_df['Emotion'].unique()\n",
    "crema_organized_df['Emotion'].unique()\n",
    "savee_metadata_df['Emotion'].unique()\n",
    "tess_metadata_df['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d485a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:43.525044Z",
     "iopub.status.busy": "2024-11-11T06:31:43.524744Z",
     "iopub.status.idle": "2024-11-11T06:31:43.536964Z",
     "shell.execute_reply": "2024-11-11T06:31:43.536061Z"
    },
    "papermill": {
     "duration": 0.025092,
     "end_time": "2024-11-11T06:31:43.538896",
     "exception": false,
     "start_time": "2024-11-11T06:31:43.513804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Emotional Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03-01-08-01-01-01-02.wav</td>\n",
       "      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n",
       "      <td>female</td>\n",
       "      <td>surprised</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03-01-01-01-01-01-02.wav</td>\n",
       "      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n",
       "      <td>female</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03-01-07-02-01-02-02.wav</td>\n",
       "      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n",
       "      <td>female</td>\n",
       "      <td>disgust</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03-01-07-01-01-02-02.wav</td>\n",
       "      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n",
       "      <td>female</td>\n",
       "      <td>disgust</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03-01-01-01-02-01-02.wav</td>\n",
       "      <td>/kaggle/input/speech-emotion-recognition-en/Ra...</td>\n",
       "      <td>female</td>\n",
       "      <td>neutral</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Filename  \\\n",
       "0  03-01-08-01-01-01-02.wav   \n",
       "1  03-01-01-01-01-01-02.wav   \n",
       "2  03-01-07-02-01-02-02.wav   \n",
       "3  03-01-07-01-01-02-02.wav   \n",
       "4  03-01-01-01-02-01-02.wav   \n",
       "\n",
       "                                            Filepath  Gender    Emotion  \\\n",
       "0  /kaggle/input/speech-emotion-recognition-en/Ra...  female  surprised   \n",
       "1  /kaggle/input/speech-emotion-recognition-en/Ra...  female    neutral   \n",
       "2  /kaggle/input/speech-emotion-recognition-en/Ra...  female    disgust   \n",
       "3  /kaggle/input/speech-emotion-recognition-en/Ra...  female    disgust   \n",
       "4  /kaggle/input/speech-emotion-recognition-en/Ra...  female    neutral   \n",
       "\n",
       "  Emotional Intensity  \n",
       "0              medium  \n",
       "1              medium  \n",
       "2                high  \n",
       "3              medium  \n",
       "4              medium  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the metadata for all dataframes!\n",
    "\n",
    "combined_metadata_df = pd.concat([ravdess_metadata_df,\n",
    "                                  crema_organized_df,\n",
    "                                  savee_metadata_df,\n",
    "                                  tess_metadata_df])\n",
    "\n",
    "combined_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477c13cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:43.561160Z",
     "iopub.status.busy": "2024-11-11T06:31:43.560878Z",
     "iopub.status.idle": "2024-11-11T06:31:43.886297Z",
     "shell.execute_reply": "2024-11-11T06:31:43.885466Z"
    },
    "papermill": {
     "duration": 0.338592,
     "end_time": "2024-11-11T06:31:43.888227",
     "exception": false,
     "start_time": "2024-11-11T06:31:43.549635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "disgust      1923\n",
       "sad          1923\n",
       "fear         1923\n",
       "happy        1923\n",
       "angry        1923\n",
       "neutral      1895\n",
       "surprised     652\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Exploratory Data Analysis\n",
    "\n",
    "sns.countplot(data = combined_metadata_df, x = 'Emotion')\n",
    "combined_metadata_df['Emotion'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b06d64",
   "metadata": {
    "papermill": {
     "duration": 0.010938,
     "end_time": "2024-11-11T06:31:43.910178",
     "exception": false,
     "start_time": "2024-11-11T06:31:43.899240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Notice how surprise category is low on numbers! We should perform data augmentation only on the training dataset to avoid data leakage**\n",
    "\n",
    "The game plan is to:\n",
    "* 1: Augment data to the surprised category to make it balanced\n",
    "* 2: Split the data into training and testing datasets\n",
    "* 3: Augment the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7230b298",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:43.934484Z",
     "iopub.status.busy": "2024-11-11T06:31:43.934167Z",
     "iopub.status.idle": "2024-11-11T06:31:43.957987Z",
     "shell.execute_reply": "2024-11-11T06:31:43.957125Z"
    },
    "papermill": {
     "duration": 0.038923,
     "end_time": "2024-11-11T06:31:43.959916",
     "exception": false,
     "start_time": "2024-11-11T06:31:43.920993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Surprised dataframe\n",
    "\n",
    "\n",
    "surprised_df = combined_metadata_df.loc[combined_metadata_df['Emotion'] == 'surprised']\n",
    "\n",
    "if AUGMENTATIONS_LOADED == False:\n",
    "    #Write Augmentation Functions and Helper Functions:\n",
    "    augmented_dir = \"augmented_surprised_samples\"\n",
    "    \n",
    "    #Delete everyfile in augmented_dirs_folder\n",
    "    if os.path.exists(augmented_dir):\n",
    "        shutil.rmtree(augmented_dir) \n",
    "    os.makedirs(augmented_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    #set random seed to be 42\n",
    "    random.seed(42)\n",
    "\n",
    "def add_white_noise(waveform, \n",
    "                    noise_level=np.random.uniform(low =  0.0001,high = 0.001)):\n",
    "    noise = torch.randn_like(waveform, device = device) * noise_level\n",
    "    return waveform + noise\n",
    "\n",
    "def time_stretch(waveform, sample_rate, rate=None):\n",
    "    if rate is None:\n",
    "        rate = np.random.uniform(0.8, 1.2)\n",
    "    waveform_np = waveform.squeeze().cpu().numpy()  # Convert to NumPy\n",
    "    stretched = librosa.effects.time_stretch(waveform_np, rate=rate)\n",
    "    return torch.tensor(stretched, device = device).unsqueeze(0)  # Convert back to tensor with channel dimension\n",
    "\n",
    "def pitch_scale(waveform, sample_rate, \n",
    "                n_steps=None):\n",
    "    if n_steps is None:\n",
    "        n_steps = np.random.uniform(low = -1, high = 1)\n",
    "    waveform_np = waveform.squeeze().cpu().numpy()  # Convert to NumPy\n",
    "    pitched = librosa.effects.pitch_shift(waveform_np, sr=sample_rate, n_steps=n_steps)\n",
    "    return torch.tensor(pitched, device = device).unsqueeze(0)  # Convert back to tensor with channel dimension\n",
    "\n",
    "def polarity_inversion(waveform):\n",
    "    return -waveform\n",
    "\n",
    "def apply_gain(waveform, gain_factor = np.random.uniform(low = 5, high = 30)):\n",
    "    gain = torchaudio.transforms.Vol(gain = gain_factor, gain_type = 'amplitude').to(device)\n",
    "    return gain(waveform)\n",
    "\n",
    "def apply_augmentations(waveform, sample_rate):\n",
    "    # List of possible augmentations\n",
    "    augmentations = [\n",
    "        lambda x: add_white_noise(x),\n",
    "        lambda x: time_stretch(x, sample_rate),\n",
    "        lambda x: pitch_scale(x, sample_rate),\n",
    "        lambda x: polarity_inversion(x),\n",
    "        lambda x: apply_gain(x)\n",
    "    ]\n",
    "    \n",
    "    # Randomly choose one or more augmentations to apply\n",
    "    num_augmentations = 3\n",
    "    selected_augmentations = random.sample(augmentations, num_augmentations)\n",
    "    \n",
    "    for augment in selected_augmentations:\n",
    "        waveform = augment(waveform)\n",
    "    return waveform\n",
    "\n",
    "\n",
    "# Assuming you have a dataframe 'surprised_df' with original \"surprised\" audio samples\n",
    "def create_augmented_samples_surprised(surprised_df, new_sample_rate = None):\n",
    "    augmented_samples = []\n",
    "    \n",
    "    for version in range(1,3):\n",
    "        # First round of augmentation (AugmentedV1)\n",
    "        for i, row in tqdm(surprised_df.iterrows(), total = len(surprised_df), desc=f\"Augmenting Version {version}\"):\n",
    "            file_path = row['Filepath']\n",
    "            file_name = row['Filename'].split('.')[0]\n",
    "            \n",
    "            augmented_file_name = f\"{file_name}_surprised_augmentedV{version}_{i}.wav\"\n",
    "            gender = row['Gender']\n",
    "            emotional_intensity = row['Emotional Intensity']\n",
    "            \n",
    "            \n",
    "            waveform, original_sr = torchaudio.load(file_path)\n",
    "            waveform = waveform.to(device)\n",
    "            \n",
    "            #print(file_name)\n",
    "\n",
    "            # Ensure waveform is 2D [1, num_samples] if it's mono\n",
    "            if waveform.ndim > 1:\n",
    "                waveform = waveform.mean(dim = 0).unsqueeze(0)\n",
    "\n",
    "            #step1= time.time()\n",
    "            # Resample if necessary\n",
    "            if original_sr != new_sample_rate and new_sample_rate is not None:\n",
    "                resampler = torchaudio.transforms.Resample(original_sr, \n",
    "                                                           new_sample_rate,\n",
    "                                                           lowpass_filter_width=16, \n",
    "                                                           resampling_method='sinc_interp_hann').to(device)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            #step2= time.time()\n",
    "            #print(f'Resample time was {step1-step2}')\n",
    "\n",
    "            \n",
    "            # Apply augmentations to create the first round of augmented samples\n",
    "            augmented_waveform = apply_augmentations(waveform, original_sr)\n",
    "\n",
    "            augmented_waveform = augmented_waveform.cpu()\n",
    "\n",
    "            # Save augmented sample to a new file\n",
    "            augmented_file_path = os.path.join(augmented_dir, augmented_file_name)\n",
    "            torchaudio.save(augmented_file_path, augmented_waveform, original_sr, channels_first = True)\n",
    "\n",
    "            # Append augmented sample details to the list\n",
    "            augmented_samples.append({\n",
    "                'Filename': augmented_file_name,\n",
    "                'Filepath': augmented_file_path,\n",
    "                'Gender': gender,\n",
    "                'Emotion': 'surprised',  # Keep the same emotion label\n",
    "                'Emotional Intensity': emotional_intensity,\n",
    "                'Augmentation_Type': f'AugmentedV{version}'  # Tag the augmentation type\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame for compatibility\n",
    "    augmented_df = pd.DataFrame(augmented_samples)\n",
    "    return augmented_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51aba7c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:43.982853Z",
     "iopub.status.busy": "2024-11-11T06:31:43.982528Z",
     "iopub.status.idle": "2024-11-11T06:31:43.987549Z",
     "shell.execute_reply": "2024-11-11T06:31:43.986678Z"
    },
    "papermill": {
     "duration": 0.018657,
     "end_time": "2024-11-11T06:31:43.989477",
     "exception": false,
     "start_time": "2024-11-11T06:31:43.970820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Save and run the augmentations \n",
    "#Augment only the surprised_df to create new samples:\n",
    "\n",
    "if AUGMENTATIONS_LOADED == False:\n",
    "    sample_rate = 24414 \n",
    "    augmented_surprised_df = create_augmented_samples_surprised(surprised_df, \n",
    "                                                                new_sample_rate = sample_rate) \n",
    "    \n",
    "    shutil.make_archive('augmented_surprised_samples', 'zip', augmented_dir)\n",
    "    \n",
    "    # Download the zip file (if using a Jupyter environment)\n",
    "    from IPython.display import FileLink\n",
    "    FileLink('augmented_surprised_samples.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85b00062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:44.012531Z",
     "iopub.status.busy": "2024-11-11T06:31:44.012231Z",
     "iopub.status.idle": "2024-11-11T06:31:44.016828Z",
     "shell.execute_reply": "2024-11-11T06:31:44.015999Z"
    },
    "papermill": {
     "duration": 0.018285,
     "end_time": "2024-11-11T06:31:44.018728",
     "exception": false,
     "start_time": "2024-11-11T06:31:44.000443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Combine the augmented surprised with the combined_metadata_df \n",
    "\n",
    "if AUGMENTATIONS_LOADED == False:\n",
    "    combined_metadata_df = pd.concat((combined_metadata_df,\n",
    "                                      augmented_surprised_df.drop('Augmentation_Type',axis = 1)))\n",
    "    \n",
    "    sns.countplot(data = combined_metadata_df, x = 'Emotion')\n",
    "    combined_metadata_df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79075ca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:44.042015Z",
     "iopub.status.busy": "2024-11-11T06:31:44.041747Z",
     "iopub.status.idle": "2024-11-11T06:31:44.051400Z",
     "shell.execute_reply": "2024-11-11T06:31:44.050527Z"
    },
    "papermill": {
     "duration": 0.023384,
     "end_time": "2024-11-11T06:31:44.053289",
     "exception": false,
     "start_time": "2024-11-11T06:31:44.029905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Splitting Training and Testing Data\n",
    "\n",
    "#gameplan:\n",
    "#1. Take nonaugmented surprised samples and make those the testing samples\n",
    "#2. Take the other emotions and create testing and training samples \n",
    "\n",
    "# Take nonaugmented surprised data for testing data\n",
    "\n",
    "\n",
    "#Create function for creating training and testing indices for other emotions\n",
    "def creating_training_testing_split(combined_metadata_df, test_size ,drop_emotion = True, **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    **kwargs can be the following:\n",
    "    emotion_to_drop = emotion\n",
    "    random_state = random_state for train_test_split\n",
    "\n",
    "    \"\"\"\n",
    "    for key,value in kwargs.items():\n",
    "        if key == 'emotion_to_drop':\n",
    "            emotion_to_drop = value\n",
    "        if key == 'random_state':\n",
    "            random_state = value\n",
    "            \n",
    "    emotions_list = list(combined_metadata_df['Emotion'].unique())\n",
    "    \n",
    "    if drop_emotion == True:\n",
    "        emotions_list.remove(emotion_to_drop)\n",
    "\n",
    "    test_indices = {}\n",
    "    train_indices = {}\n",
    "\n",
    "    training_df = pd.DataFrame(columns = combined_metadata_df.columns)\n",
    "    testing_df = pd.DataFrame(columns = combined_metadata_df.columns)\n",
    "\n",
    "    for emotion in emotions_list:\n",
    "        emotion_df = combined_metadata_df[combined_metadata_df['Emotion'] == emotion]\n",
    "        \n",
    "        #test_size = 652\n",
    "\n",
    "        train_idxs, test_idxs= train_test_split(range(len(emotion_df)), \n",
    "                                                 test_size = test_size, \n",
    "                                                 random_state = SEED)\n",
    "        #Append to dictionary  \n",
    "        test_indices[emotion] =  test_idxs\n",
    "        train_indices[emotion] =  train_idxs\n",
    "\n",
    "        emotion_df_train = emotion_df.iloc[train_indices[emotion],:]\n",
    "        emotion_df_test = emotion_df.iloc[test_indices[emotion],:]\n",
    "\n",
    "        training_df = pd.concat((training_df,emotion_df_train),axis = 0)\n",
    "        testing_df = pd.concat((testing_df,emotion_df_test),axis = 0)\n",
    "\n",
    "    return training_df, testing_df, train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c44e5ff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:44.076957Z",
     "iopub.status.busy": "2024-11-11T06:31:44.076405Z",
     "iopub.status.idle": "2024-11-11T06:31:44.086584Z",
     "shell.execute_reply": "2024-11-11T06:31:44.085751Z"
    },
    "papermill": {
     "duration": 0.024321,
     "end_time": "2024-11-11T06:31:44.088510",
     "exception": false,
     "start_time": "2024-11-11T06:31:44.064189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create the training splits by calling the function\n",
    "\n",
    "if AUGMENTATIONS_LOADED == False:\n",
    "    surprised_df = combined_metadata_df.loc[combined_metadata_df['Emotion'] == 'surprised']\n",
    "    \n",
    "    nonaugmented_surprised_df = []\n",
    "    augmented_surprised_df = []\n",
    "    \n",
    "    for i,row in surprised_df.iterrows():\n",
    "        \n",
    "        gender = row['Gender']\n",
    "        filename = row['Filename'] \n",
    "        filepath = row['Filepath']\n",
    "        emotional_int = row['Emotional Intensity']\n",
    "        emotion = row['Emotion']\n",
    "        \n",
    "        if 'surprised_augmented' not in row['Filename']:\n",
    "            \n",
    "            nonaugmented_surprised_df.append({\n",
    "                'Filename': filename,\n",
    "                'Filepath': filepath,\n",
    "                'Gender': gender,\n",
    "                'Emotion': emotion,\n",
    "                'Emotional Intensity': emotional_int\n",
    "            })\n",
    "        else:\n",
    "            augmented_surprised_df.append({\n",
    "                'Filename': filename,\n",
    "                'Filepath': filepath,\n",
    "                'Gender': gender,\n",
    "                'Emotion': emotion,\n",
    "                'Emotional Intensity': emotional_int\n",
    "            })\n",
    "            \n",
    "    nonaugmented_surprised_df = pd.DataFrame(nonaugmented_surprised_df)\n",
    "    augmented_surprised_df = pd.DataFrame(augmented_surprised_df)\n",
    "    testing_samples = nonaugmented_surprised_df.shape[0]\n",
    "    \n",
    "    \n",
    "    training_df,testing_df,_,_ = creating_training_testing_split(combined_metadata_df, \n",
    "                                    test_size = testing_samples ,\n",
    "                                    drop_emotion = True, \n",
    "                                    emotion_to_drop = 'surprised')\n",
    "    \n",
    "    #append the surprised to the training dataframes and testing dataframes\n",
    "    training_df = pd.concat((augmented_surprised_df,training_df),axis = 0)\n",
    "    testing_df = pd.concat((nonaugmented_surprised_df,testing_df),axis = 0)\n",
    "    \n",
    "    \n",
    "    training_df.shape\n",
    "    \n",
    "    sns.countplot(data = training_df, x = 'Emotion')\n",
    "    combined_metadata_df['Emotion'].value_counts()\n",
    "    plt.show()\n",
    "    \n",
    "    sns.countplot(data = testing_df, x = 'Emotion')\n",
    "    combined_metadata_df['Emotion'].value_counts()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c301b1",
   "metadata": {
    "papermill": {
     "duration": 0.010698,
     "end_time": "2024-11-11T06:31:44.110071",
     "exception": false,
     "start_time": "2024-11-11T06:31:44.099373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Augment the training samples excluding the surprised category (since those are already augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee44fa1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:44.133764Z",
     "iopub.status.busy": "2024-11-11T06:31:44.133073Z",
     "iopub.status.idle": "2024-11-11T06:31:44.144551Z",
     "shell.execute_reply": "2024-11-11T06:31:44.143751Z"
    },
    "papermill": {
     "duration": 0.025631,
     "end_time": "2024-11-11T06:31:44.146480",
     "exception": false,
     "start_time": "2024-11-11T06:31:44.120849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Augment the training data\n",
    "\n",
    "if AUGMENTATIONS_LOADED == False:\n",
    "    non_surprised_df = combined_metadata_df.loc[combined_metadata_df['Emotion'] != 'surprised']\n",
    "    \n",
    "    #Write Augmentation Functions and Helper Functions:\n",
    "    augmented_dir = \"augmented_training_samples\"\n",
    "    \n",
    "    #Delete everyfile in augmented_dirs_folder\n",
    "    if os.path.exists(augmented_dir):\n",
    "        shutil.rmtree(augmented_dir) \n",
    "    os.makedirs(augmented_dir, exist_ok=True)\n",
    "\n",
    "def create_training_augmentations(non_surprised_df, new_sample_rate):\n",
    "    augmented_training_samples = []\n",
    "    # First round of augmentation training samples (no surprised) (AugmentedV1)\n",
    "    \n",
    "    for i, row in tqdm(non_surprised_df.iterrows(),total = len(non_surprised_df),desc=f\"Augmenting Version 1\"):\n",
    "        file_path = row['Filepath']\n",
    "        file_name = row['Filename'].split('.')[0]\n",
    "\n",
    "        augmented_file_name = f\"{file_name}_augmented_training_V1_{i}.wav\"\n",
    "        gender = row['Gender']\n",
    "        emotional_intensity = row['Emotional Intensity']\n",
    "        emotion = row['Emotion']\n",
    "\n",
    "\n",
    "        waveform, original_sr = torchaudio.load(file_path)\n",
    "        waveform = waveform.to(device)\n",
    "        #print(file_name)\n",
    "\n",
    "        # Ensure waveform is 2D [1, num_samples] if it's mono\n",
    "        if waveform.ndim > 1:\n",
    "            waveform = waveform.mean(dim = 0).unsqueeze(0)\n",
    "            \n",
    "         # Resample if necessary\n",
    "        if original_sr != new_sample_rate and new_sample_rate is not None:\n",
    "            resampler = torchaudio.transforms.Resample(original_sr, \n",
    "                                                       new_sample_rate,\n",
    "                                                       lowpass_filter_width=16, \n",
    "                                                       resampling_method='sinc_interp_hann').to(device)\n",
    "            waveform = resampler(waveform)\n",
    "            \n",
    "        # Apply augmentations to create the first round of augmented samples\n",
    "        augmented_waveform = apply_augmentations(waveform, original_sr)\n",
    "        augmented_waveform = augmented_waveform.cpu()\n",
    "\n",
    "        # Save augmented sample to a new file\n",
    "        augmented_file_path = os.path.join(augmented_dir, augmented_file_name)\n",
    "        torchaudio.save(augmented_file_path, augmented_waveform, original_sr, channels_first = True)\n",
    "\n",
    "        # Append augmented sample details to the list\n",
    "        augmented_training_samples.append({\n",
    "            'Filename': augmented_file_name,\n",
    "            'Filepath': augmented_file_path,\n",
    "            'Gender': gender,\n",
    "            'Emotion': emotion,  # Keep the same emotion label\n",
    "            'Emotional Intensity': emotional_intensity,  \n",
    "            'Augmentation_Type': f'AugmentedV'  # Tag the augmentation type\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame for compatibility\n",
    "    augmented_training_df = pd.DataFrame(augmented_training_samples)\n",
    "    return augmented_training_df\n",
    "\n",
    "if AUGMENTATIONS_LOADED == False:\n",
    "    augmented_training_df = create_training_augmentations(non_surprised_df,new_sample_rate = 24414) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bb18b47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:44.169526Z",
     "iopub.status.busy": "2024-11-11T06:31:44.169024Z",
     "iopub.status.idle": "2024-11-11T06:31:44.173723Z",
     "shell.execute_reply": "2024-11-11T06:31:44.172848Z"
    },
    "papermill": {
     "duration": 0.01821,
     "end_time": "2024-11-11T06:31:44.175579",
     "exception": false,
     "start_time": "2024-11-11T06:31:44.157369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if AUGMENTATIONS_LOADED == False:\n",
    "    # Compress the augmented directory\n",
    "    shutil.make_archive('augmented_training_samples', 'zip', augmented_dir)\n",
    "    \n",
    "    # Download the zip file (if using a Jupyter environment)\n",
    "    from IPython.display import FileLink\n",
    "    FileLink('augmented_training_samples.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8729abb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:44.198161Z",
     "iopub.status.busy": "2024-11-11T06:31:44.197897Z",
     "iopub.status.idle": "2024-11-11T06:31:44.202499Z",
     "shell.execute_reply": "2024-11-11T06:31:44.201653Z"
    },
    "papermill": {
     "duration": 0.018085,
     "end_time": "2024-11-11T06:31:44.204369",
     "exception": false,
     "start_time": "2024-11-11T06:31:44.186284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if AUGMENTATIONS_LOADED == False:\n",
    "    #Create the testing samples dataset for kaggle\n",
    "    testing_df.to_csv('testing_df.csv', index=False)\n",
    "    augmented_training_df.to_csv('augmented_training_df.csv', index = False)\n",
    "    print(FileLink('testing_df.csv'))\n",
    "    print(FileLink('augmented_training_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "694a69be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:44.227106Z",
     "iopub.status.busy": "2024-11-11T06:31:44.226818Z",
     "iopub.status.idle": "2024-11-11T06:31:44.233709Z",
     "shell.execute_reply": "2024-11-11T06:31:44.232849Z"
    },
    "papermill": {
     "duration": 0.020353,
     "end_time": "2024-11-11T06:31:44.235568",
     "exception": false,
     "start_time": "2024-11-11T06:31:44.215215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a folder called Metadata and Augmentations \n",
    "\n",
    "if AUGMENTATIONS_LOADED == False:\n",
    "\n",
    "    # Step 1: Create a folder for \"Metadata and Augmentations\"\n",
    "    output_folder = \"Metadata_and_Augmentations\"\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)  # Remove the folder if it exists to start fresh\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Step 2: Copy the necessary files to the folder\n",
    "    shutil.copy('augmented_training_df.csv', output_folder)\n",
    "    shutil.copy('testing_df.csv', output_folder)\n",
    "    \n",
    "    # Step 3: Unzip the augmented training and surprised samples folders\n",
    "    with ZipFile('augmented_training_samples.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.join(output_folder, 'augmented_training_samples'))\n",
    "    \n",
    "    with ZipFile('augmented_surprised_samples.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.join(output_folder, 'augmented_surprised_samples'))\n",
    "    \n",
    "    # Step 4: Create a zip file containing the \"Metadata and Augmentations\" folder\n",
    "    output_zip_file = 'Metadata_and_Augmentations.zip'\n",
    "    shutil.make_archive('Metadata_and_Augmentations', 'zip', output_folder)\n",
    "    \n",
    "    # Step 5: Provide a download link (if using a Jupyter notebook environment)\n",
    "    from IPython.display import FileLink\n",
    "    FileLink(output_zip_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e89322a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:44.258822Z",
     "iopub.status.busy": "2024-11-11T06:31:44.258495Z",
     "iopub.status.idle": "2024-11-11T06:31:44.266319Z",
     "shell.execute_reply": "2024-11-11T06:31:44.265472Z"
    },
    "papermill": {
     "duration": 0.021402,
     "end_time": "2024-11-11T06:31:44.268277",
     "exception": false,
     "start_time": "2024-11-11T06:31:44.246875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Path (<tt>Metadata_and_Augmentations.zip</tt>) doesn't exist. It may still be in the process of being generated, or you may have the incorrect path."
      ],
      "text/plain": [
       "/kaggle/input/metadata-and-augmentations/Metadata_and_Augmentations.zip"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()\n",
    "from IPython.display import FileLink\n",
    "FileLink('Metadata_and_Augmentations.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d0f78",
   "metadata": {
    "papermill": {
     "duration": 0.010816,
     "end_time": "2024-11-11T06:31:44.289980",
     "exception": false,
     "start_time": "2024-11-11T06:31:44.279164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# !End of Data Preprocessing \n",
    "**(Hint Select this cell, go Run -> run before and it should run all cells before this current cell)**\n",
    "\n",
    "**If you have ran this notebook, you should get 3 new datasets**:\n",
    "* augmented-surprised-samples\n",
    "* augmented-training-samples\n",
    "* testing-samples-metadata-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b6af191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:44.313182Z",
     "iopub.status.busy": "2024-11-11T06:31:44.312924Z",
     "iopub.status.idle": "2024-11-11T06:31:44.339511Z",
     "shell.execute_reply": "2024-11-11T06:31:44.338672Z"
    },
    "papermill": {
     "duration": 0.040693,
     "end_time": "2024-11-11T06:31:44.341513",
     "exception": false,
     "start_time": "2024-11-11T06:31:44.300820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# These datasets are the result of data preprocessing cells you ran before this cell. \n",
    "\n",
    "#We need to make sure the audio files are the same size when creating the dataloader:\n",
    "\n",
    "#Helper functions:\n",
    "def get_max_audio_length(metadata_df):\n",
    "    \"\"\"Calculate the maximum length of audio samples in the dataset.\"\"\"\n",
    "    start = time.time()\n",
    "    max_length = 0\n",
    "    for file_path in metadata_df['Filepath']:\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        num_samples = waveform.shape[1]  # Number of samples in the waveform\n",
    "        if num_samples > max_length:\n",
    "            max_length = num_samples\n",
    "    end = time.time()\n",
    "    print (f'Max_length_found: {max_length}. Took {start - end} seconds')\n",
    "    return max_length\n",
    "\n",
    "def same_length_batch(batch):\n",
    "    # Collate function to handle variable-length sequences\n",
    "\n",
    "    # Extract waveforms, emotions, and genders from the batch\n",
    "    waveforms = [item['waveform'].squeeze(0) for item in batch]  # Remove channel dimension if present (it's all mono anyways!)\n",
    "    emotions = torch.stack([item['emotion'] for item in batch]) #example [0,1,2]\n",
    "    genders = torch.stack([item['gender'] for item in batch]) #same as above\n",
    "    intensity = torch.stack([item['intensity'] for item in batch])\n",
    "    sample_rate = torch.stack([item['sample rate'] for item in batch])\n",
    "\n",
    "    \n",
    "    # Pad all waveforms to the same length (of the longest in the batch) ****THIS MEANS WE WILL HAVE TO MAKE THE NN VARIABLE LENGTH DEPENDING ON THE BATCH!\n",
    "    waveforms_padded = pad_sequence(waveforms, batch_first=True) #put the batch dimension first!\n",
    "    \n",
    "    # Return padded waveforms and corresponding labels\n",
    "    return {'waveform': waveforms_padded, \n",
    "            'emotion': emotions, \n",
    "            'gender': genders,\n",
    "            'vocal channel': vocal_channels,\n",
    "            'intensity':intensity,\n",
    "            'statement': statement,\n",
    "            'sample rate':sample_rate}\n",
    "\n",
    "#Create the custom pytorch dataset\n",
    "\n",
    "class Emotion_Classification_Dataset(Dataset):\n",
    "    def __init__(self, metadata_df, \n",
    "                 transformations=None, \n",
    "                 same_length_all = True, \n",
    "                 target_length = None,\n",
    "                 target_sr = None,\n",
    "                 augment_surprised=False,\n",
    "                 device = device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metadata_df (DataFrame): DataFrame containing file paths and labels.\n",
    "            target_length (int): Target length for all audio samples in number of samples.\n",
    "            transformations (callable, optional): Optional list of transformations to be applied on a sample (e.g., audio augmentation).\n",
    "            same_length_all (Boolean): If True, enforce same length for all audio samples.\n",
    "        \"\"\"\n",
    "        self.metadata_df = metadata_df\n",
    "        self.transformations = transformations\n",
    "        self.same_length_all = same_length_all #Boolean\n",
    "        self.target_length = target_length\n",
    "        self.augment_surprised = augment_surprised  # Boolean for augmenting \"surprised\" category\n",
    "        self.target_sr = target_sr\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "    \n",
    "        # Get file path and labels from the DataFrame\n",
    "        file_path = self.metadata_df.iloc[idx]['Filepath']\n",
    "        emotion_label = self.metadata_df.iloc[idx]['Emotion']\n",
    "        gender_label = self.metadata_df.iloc[idx]['Gender']\n",
    "        intensity = self.metadata_df.iloc[idx]['Emotional Intensity']\n",
    "\n",
    "        step1_time = time.time()\n",
    "        print(f\"Step 1 (Fetching metadata): {step1_time - start_time:.4f} seconds\")\n",
    "        \n",
    "        # Load audio file (torchaudio returns waveform and sample rate)\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        step2_time = time.time()\n",
    "        print(f\"Step 2 (Loading audio): {step2_time - step1_time:.4f} seconds\")\n",
    "\n",
    "        \n",
    "        waveform = waveform.to(device)\n",
    "        \n",
    "        #Resample the data\n",
    "        #waveform = self.resample_if_necessary(waveform, sample_rate) \n",
    "        \n",
    "        step3_time = time.time()\n",
    "        print(f\"Step 3 (Resampling): {step3_time - step2_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "        #Pad the data to make the same length\n",
    "        if self.same_length_all and self.target_length is not None:\n",
    "            waveform = self.pad_or_trim_waveform(waveform, self.target_length)\n",
    "            \n",
    "        step4_time = time.time()\n",
    "        print(f\"Step 4 (Padding/Trimming): {step4_time - step3_time:.4f} seconds\")\n",
    "\n",
    "        #waveform.cpu()\n",
    "        waveform_feature_dict = {'original waveform':waveform}\n",
    "        \n",
    "        # Extract features based on transformations\n",
    "        if self.transformations:\n",
    "            #waveform = waveform.to(device)\n",
    "            for transformation in self.transformations:\n",
    "                waveform_feature = transformation(waveform)\n",
    "                waveform_feature_dict[transformation.__class__.__name__] = waveform_feature#.cpu()\n",
    "                \n",
    "        step5_time = time.time()\n",
    "        print(f\"Step 5 (Transformations): {step5_time - step4_time:.4f} seconds\")\n",
    "\n",
    "        # Convert labels to tensors or numerical values\n",
    "        emotion_mapping = {\n",
    "            \"neutral\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3,\n",
    "            \"fear\": 4, \"disgust\": 5, \"surprised\": 6\n",
    "        }\n",
    "        gender_mapping = {\"male\": 0, \"female\": 1}\n",
    "        intensity_mapping = {\"low\": 0, \"medium\": 1, \"high\": 2, \"unknown\": 3}\n",
    "\n",
    "        step6_time = time.time()\n",
    "        print(f\"Step 6 (Label conversion): {step6_time - step5_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "        emotion_tensor = torch.tensor(emotion_mapping[emotion_label])\n",
    "        gender_tensor = torch.tensor(gender_mapping[gender_label])\n",
    "        intensity_tensor = torch.tensor(intensity_mapping[intensity])\n",
    "        # You can return the labels as part of a dictionary\n",
    "        sample = {'waveform_features': waveform_feature_dict, \n",
    "                  'emotion': emotion_tensor, \n",
    "                  'gender': gender_tensor,\n",
    "                  'intensity': intensity_tensor,\n",
    "                  'sample rate':torch.tensor(sample_rate)}\n",
    "\n",
    "        total_time = time.time()\n",
    "        print(f\"Total time for __getitem__: {total_time - start_time:.4f} seconds\")\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def pad_or_trim_waveform(self, waveform, target_length):\n",
    "        \"\"\"Pad or trim waveform to a fixed target length in samples.\"\"\"\n",
    "        num_samples = waveform.shape[1]  # waveform shape is (channels, num_samples)\n",
    "\n",
    "        if num_samples < target_length:\n",
    "            # Pad if the waveform is shorter than target length\n",
    "            padding = target_length - num_samples\n",
    "            waveform = F.pad(waveform, (0, padding))#.to(device) #pad the left with 0 0s and pad the right with padding amount of 0s\n",
    "        elif num_samples > target_length:\n",
    "            # Trim if the waveform is longer than target length\n",
    "            waveform = waveform[:, :target_length]\n",
    "\n",
    "        return waveform\n",
    "    \n",
    "    # def resample_if_necessary(self, waveform, sr):\n",
    "    #     if sr != self.target_sr:\n",
    "    #         resampler = torchaudio.transforms.Resample(sr, self.target_sr)\n",
    "    #         waveform = resampler(waveform) \n",
    "            \n",
    "    #     return waveform\n",
    "\n",
    "    def resample_if_necessary(self, waveform, sr):\n",
    "        if sr != self.target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sr)#.to(device) # Move to GPU\n",
    "            waveform = resampler(waveform)\n",
    "        return waveform\n",
    "\n",
    "\n",
    "#not in the dataloader class btw....\n",
    "def load_dataset(metadata_df,\n",
    "                 same_length_all = True,\n",
    "                 sample_rate = 24414,\n",
    "                 seconds_of_audio = 3,\n",
    "                transformations = None):\n",
    "\n",
    "    # def worker_init_fn(worker_id):\n",
    "    #     random.seed(SEED)\n",
    "    #     np.random.seed(SEED)\n",
    "    \n",
    "    if same_length_all:\n",
    "        \n",
    "        #max length of our audio dataset (without any augmentation is 314818) \n",
    "        #max_length = get_max_audio_length(combined_metadata_df)\n",
    "        #max_length = 314818 #TRUE MAX LENGTH\n",
    "        \n",
    "        max_length = sample_rate * seconds_of_audio\n",
    "        combined_dataset = Emotion_Classification_Dataset(metadata_df=metadata_df,\n",
    "                                                transformations = transformations,\n",
    "                                                same_length_all = True,\n",
    "                                                target_length = max_length,\n",
    "                                                target_sr = 24414)\n",
    "        # dataloader = DataLoader(combined_dataset, \n",
    "        #                         batch_size=16, \n",
    "        #                         shuffle=True,  \n",
    "        #                         num_workers = 1)#,persistent_workers=True)\n",
    "\n",
    "        dataloader = DataLoader(combined_dataset, \n",
    "                                batch_size=16, \n",
    "                                shuffle=True)\n",
    "        \n",
    "    else:\n",
    "        combined_dataset = Emotion_Classification_Dataset(metadata_df=metadata_df,\n",
    "                                                transformations = transformations,\n",
    "                                                same_length_all = False,\n",
    "                                                target_sr = 24414)\n",
    "        dataloader = DataLoader(combined_dataset, \n",
    "                                batch_size=16, \n",
    "                                shuffle=True, \n",
    "                                collate_fn=same_length_batch, \n",
    "                                num_workers = 8,\n",
    "                                persistent_workers=True,\n",
    "                                worker_init_fn=worker_init_fn)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a71416f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:44.365385Z",
     "iopub.status.busy": "2024-11-11T06:31:44.365099Z",
     "iopub.status.idle": "2024-11-11T06:31:45.833760Z",
     "shell.execute_reply": "2024-11-11T06:31:45.832810Z"
    },
    "papermill": {
     "duration": 1.48376,
     "end_time": "2024-11-11T06:31:45.836397",
     "exception": false,
     "start_time": "2024-11-11T06:31:44.352637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2668144702911377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/720 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 (Fetching metadata): 0.0005 seconds\n",
      "Step 2 (Loading audio): 0.0790 seconds\n",
      "Step 3 (Resampling): 0.0002 seconds\n",
      "Step 4 (Padding/Trimming): 0.0193 seconds\n",
      "Step 5 (Transformations): 0.8084 seconds\n",
      "Step 6 (Label conversion): 0.0001 seconds\n",
      "Total time for __getitem__: 0.9079 seconds\n",
      "Step 1 (Fetching metadata): 0.0003 seconds\n",
      "Step 2 (Loading audio): 0.0199 seconds\n",
      "Step 3 (Resampling): 0.0002 seconds\n",
      "Step 4 (Padding/Trimming): 0.0000 seconds\n",
      "Step 5 (Transformations): 0.0006 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0212 seconds\n",
      "Step 1 (Fetching metadata): 0.0003 seconds\n",
      "Step 2 (Loading audio): 0.0158 seconds\n",
      "Step 3 (Resampling): 0.0002 seconds\n",
      "Step 4 (Padding/Trimming): 0.0001 seconds\n",
      "Step 5 (Transformations): 0.0005 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0169 seconds\n",
      "Step 1 (Fetching metadata): 0.0002 seconds\n",
      "Step 2 (Loading audio): 0.0146 seconds\n",
      "Step 3 (Resampling): 0.0001 seconds\n",
      "Step 4 (Padding/Trimming): 0.0001 seconds\n",
      "Step 5 (Transformations): 0.0004 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0155 seconds\n",
      "Step 1 (Fetching metadata): 0.0002 seconds\n",
      "Step 2 (Loading audio): 0.0160 seconds\n",
      "Step 3 (Resampling): 0.0002 seconds\n",
      "Step 4 (Padding/Trimming): 0.0001 seconds\n",
      "Step 5 (Transformations): 0.0007 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0172 seconds\n",
      "Step 1 (Fetching metadata): 0.0002 seconds\n",
      "Step 2 (Loading audio): 0.0164 seconds\n",
      "Step 3 (Resampling): 0.0002 seconds\n",
      "Step 4 (Padding/Trimming): 0.0001 seconds\n",
      "Step 5 (Transformations): 0.0005 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0174 seconds\n",
      "Step 1 (Fetching metadata): 0.0002 seconds\n",
      "Step 2 (Loading audio): 0.0153 seconds\n",
      "Step 3 (Resampling): 0.0001 seconds\n",
      "Step 4 (Padding/Trimming): 0.0001 seconds\n",
      "Step 5 (Transformations): 0.0004 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0163 seconds\n",
      "Step 1 (Fetching metadata): 0.0002 seconds\n",
      "Step 2 (Loading audio): 0.0158 seconds\n",
      "Step 3 (Resampling): 0.0002 seconds\n",
      "Step 4 (Padding/Trimming): 0.0001 seconds\n",
      "Step 5 (Transformations): 0.0004 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0167 seconds\n",
      "Step 1 (Fetching metadata): 0.0002 seconds\n",
      "Step 2 (Loading audio): 0.0132 seconds\n",
      "Step 3 (Resampling): 0.0001 seconds\n",
      "Step 4 (Padding/Trimming): 0.0001 seconds\n",
      "Step 5 (Transformations): 0.0004 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0141 seconds\n",
      "Step 1 (Fetching metadata): 0.0002 seconds\n",
      "Step 2 (Loading audio): 0.0158 seconds\n",
      "Step 3 (Resampling): 0.0001 seconds\n",
      "Step 4 (Padding/Trimming): 0.0001 seconds\n",
      "Step 5 (Transformations): 0.0004 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0167 seconds\n",
      "Step 1 (Fetching metadata): 0.0002 seconds\n",
      "Step 2 (Loading audio): 0.0162 seconds\n",
      "Step 3 (Resampling): 0.0002 seconds\n",
      "Step 4 (Padding/Trimming): 0.0001 seconds\n",
      "Step 5 (Transformations): 0.0007 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0174 seconds\n",
      "Step 1 (Fetching metadata): 0.0002 seconds\n",
      "Step 2 (Loading audio): 0.0156 seconds\n",
      "Step 3 (Resampling): 0.0002 seconds\n",
      "Step 4 (Padding/Trimming): 0.0000 seconds\n",
      "Step 5 (Transformations): 0.0004 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0165 seconds\n",
      "Step 1 (Fetching metadata): 0.0002 seconds\n",
      "Step 2 (Loading audio): 0.0155 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/720 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 (Resampling): 0.0005 seconds\n",
      "Step 4 (Padding/Trimming): 0.0004 seconds\n",
      "Step 5 (Transformations): 0.0010 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0176 seconds\n",
      "Step 1 (Fetching metadata): 0.0003 seconds\n",
      "Step 2 (Loading audio): 0.0155 seconds\n",
      "Step 3 (Resampling): 0.0001 seconds\n",
      "Step 4 (Padding/Trimming): 0.0001 seconds\n",
      "Step 5 (Transformations): 0.0004 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0165 seconds\n",
      "Step 1 (Fetching metadata): 0.0002 seconds\n",
      "Step 2 (Loading audio): 0.0163 seconds\n",
      "Step 3 (Resampling): 0.0002 seconds\n",
      "Step 4 (Padding/Trimming): 0.0000 seconds\n",
      "Step 5 (Transformations): 0.0004 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0172 seconds\n",
      "Step 1 (Fetching metadata): 0.0002 seconds\n",
      "Step 2 (Loading audio): 0.0159 seconds\n",
      "Step 3 (Resampling): 0.0002 seconds\n",
      "Step 4 (Padding/Trimming): 0.0001 seconds\n",
      "Step 5 (Transformations): 0.0004 seconds\n",
      "Step 6 (Label conversion): 0.0000 seconds\n",
      "Total time for __getitem__: 0.0168 seconds\n",
      "torch.Size([16, 1, 73242]) tensor([3, 4, 4, 1, 2, 5, 0, 2, 4, 4, 5, 3, 0, 2, 2, 0]) tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1]) tensor([3, 2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3]) tensor([16000, 48000, 16000, 24414, 24414, 16000, 24414, 16000, 16000, 16000,\n",
      "        24414, 16000, 16000, 16000, 48000, 16000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "transformations = [T.MFCC(sample_rate = 24414,\n",
    "                         n_mfcc = 40,\n",
    "                         log_mels = True,\n",
    "                         melkwargs = {\"n_fft\": 1024, \"hop_length\": 512, \"n_mels\": 64}).to(device)]\n",
    "\n",
    "dataloader = load_dataset(metadata_df = augmented_training_df,\n",
    "                          same_length_all=True,\n",
    "                          seconds_of_audio = 3,\n",
    "                          transformations = transformations) \n",
    "end = time.time()\n",
    "\n",
    "print(end - start)\n",
    "# Iterate over the DataLoader (Print one iteration of the batch!)\n",
    "for batch in tqdm(dataloader):\n",
    "    waveforms = batch['waveform_features']\n",
    "    emotions = batch['emotion']\n",
    "    genders = batch['gender']\n",
    "    sample_rate = batch['sample rate']\n",
    "    intensity = batch['intensity']\n",
    "    print(waveforms['original waveform'].shape, emotions, genders,intensity,sample_rate)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6499dd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:45.861646Z",
     "iopub.status.busy": "2024-11-11T06:31:45.861308Z",
     "iopub.status.idle": "2024-11-11T06:31:45.867063Z",
     "shell.execute_reply": "2024-11-11T06:31:45.866217Z"
    },
    "papermill": {
     "duration": 0.02073,
     "end_time": "2024-11-11T06:31:45.869049",
     "exception": false,
     "start_time": "2024-11-11T06:31:45.848319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 73242])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Play audio file\n",
    "#Audio(batch['waveform_features'][5].squeeze(), rate = 24414)\n",
    "\n",
    "batch['waveform_features']['original waveform'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26395480",
   "metadata": {
    "papermill": {
     "duration": 0.01139,
     "end_time": "2024-11-11T06:31:45.892175",
     "exception": false,
     "start_time": "2024-11-11T06:31:45.880785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1.5 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f27d0bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:31:45.917809Z",
     "iopub.status.busy": "2024-11-11T06:31:45.917503Z",
     "iopub.status.idle": "2024-11-11T06:33:12.432835Z",
     "shell.execute_reply": "2024-11-11T06:33:12.431656Z"
    },
    "papermill": {
     "duration": 86.54258,
     "end_time": "2024-11-11T06:33:12.446926",
     "exception": false,
     "start_time": "2024-11-11T06:31:45.904346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.40293979644775\n"
     ]
    }
   ],
   "source": [
    "# import addtional packages\n",
    "from scipy.signal import convolve\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Load in preprocessed dataframes\n",
    "training = pd.read_csv('/kaggle/input/metadata-and-augmentations/augmented_training_df.csv')\n",
    "testing = pd.read_csv('/kaggle/input/metadata-and-augmentations/testing_df.csv')\n",
    "\n",
    "# Make dataframes to store extracted feature values\n",
    "training_features = {\n",
    "    'Pitch - Mean': [0] * len(training),\n",
    "    'Pitch - Coefficient of Variation': [0] * len(training),\n",
    "    'Pitch - 20th Percentile': [0] * len(training),\n",
    "    'Pitch - 50th Percentile': [0] * len(training),\n",
    "    'Pitch - 80th Percentile': [0] * len(training),\n",
    "    'Pitch - Range between 20th and 80th Percentile': [0] * len(training),\n",
    "    'Pitch - Mean Rising Part': [0] * len(training),\n",
    "    'Pitch - Standard Deviation Rising Part': [0] * len(training),\n",
    "    'Pitch - Mean Falling Part': [0] * len(training),\n",
    "    'Pitch - Standard Deviation Falling Part': [0] * len(training),\n",
    "    'Jitter - Mean': [0] * len(training),\n",
    "    'Jitter - Coefficient of Variation': [0] * len(training),\n",
    "    'MFCC - Mean MFCC1': [0] * len(training),\n",
    "    'MFCC - Mean MFCC2': [0] * len(training),\n",
    "    'MFCC - Mean MFCC3': [0] * len(training),\n",
    "    'MFCC - Mean MFCC4': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC1': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC2': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC3': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC4': [0] * len(training),\n",
    "    'MFCC - Mean MFCC1 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Mean MFCC2 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Mean MFCC3 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Mean MFCC4 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC1 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC2 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC3 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC4 Voiced Region': [0] * len(training)\n",
    "}\n",
    "\n",
    "training_features = pd.DataFrame(training_features, dtype = float)\n",
    "\n",
    "testing_features = {\n",
    "    'Pitch - Mean': [0] * len(training),\n",
    "    'Pitch - Coefficient of Variation': [0] * len(training),\n",
    "    'Pitch - 20th Percentile': [0] * len(training),\n",
    "    'Pitch - 50th Percentile': [0] * len(training),\n",
    "    'Pitch - 80th Percentile': [0] * len(training),\n",
    "    'Pitch - Range between 20th and 80th Percentile': [0] * len(training),\n",
    "    'Pitch - Mean Rising Part': [0] * len(training),\n",
    "    'Pitch - Standard Deviation Rising Part': [0] * len(training),\n",
    "    'Pitch - Mean Falling Part': [0] * len(training),\n",
    "    'Pitch - Standard Deviation Falling Part': [0] * len(training),\n",
    "    'Jitter - Mean': [0] * len(training),\n",
    "    'Jitter - Coefficient of Variation': [0] * len(training),\n",
    "    'MFCC - Mean MFCC1': [0] * len(training),\n",
    "    'MFCC - Mean MFCC2': [0] * len(training),\n",
    "    'MFCC - Mean MFCC3': [0] * len(training),\n",
    "    'MFCC - Mean MFCC4': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC1': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC2': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC3': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC4': [0] * len(training),\n",
    "    'MFCC - Mean MFCC1 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Mean MFCC2 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Mean MFCC3 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Mean MFCC4 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC1 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC2 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC3 Voiced Region': [0] * len(training),\n",
    "    'MFCC - Coefficient of Variation MFCC4 Voiced Region': [0] * len(training)\n",
    "}\n",
    "\n",
    "testing_features = pd.DataFrame(testing_features, dtype = float)\n",
    "\n",
    "\n",
    "\n",
    "# Conduct feature extraction for each audio file in the training and in the testing dataset\n",
    "# for i in range(0, len(training)):\n",
    "for i in range(0, 10):\n",
    "    \n",
    "    # Load in the audio file\n",
    "    path = training[\"Filepath\"].iloc[i]\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "\n",
    "    \n",
    "\n",
    "    # Pitch extraction\n",
    "    # Convert the waveform to numpy (librosa works with numpy arrays)\n",
    "    waveform_np = waveform.numpy()\n",
    "\n",
    "    # Use librosa to extract the pitch\n",
    "    # By default, the window size is usually 2048 samples, and the hop length (the distance between consecutive windows) is typically 512 samples.\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(waveform_np, fmin=librosa.note_to_hz('A0'), fmax=librosa.note_to_hz('C8'))\n",
    "    \n",
    "    # Convert the pitch (f0) to a semitone scale starting at 27.5 Hz (A0)\n",
    "    f0_semitones = 12 * np.log2(f0 / 27.5)\n",
    "\n",
    "    # Remove NaN values (no pitch detected)\n",
    "    f0_semitones_no_nan = f0_semitones[voiced_flag]  # only consider voiced frames\n",
    "\n",
    "    # Apply symmetric moving average filter (3 frames long)\n",
    "    # The window size is 3, so the filter is [1/3, 1/3, 1/3] for a symmetric average.\n",
    "    window = np.ones(3) / 3  # symmetric moving average filter\n",
    "    smoothed_f0_semitones_no_nan = convolve(f0_semitones_no_nan, window, mode='same')\n",
    "\n",
    "    # Arithmetic Mean (AM) - pitch funtional #1\n",
    "    mean_smoothed_f0_semitones_no_nan = np.mean(smoothed_f0_semitones_no_nan)\n",
    "    training_features.loc[i, \"Pitch - Mean\"] = mean_smoothed_f0_semitones_no_nan\n",
    "\n",
    "    # Coefficient of Variation (CV) - pitch functional #2\n",
    "    std_smoothed_f0_semitones_no_nan = np.std(smoothed_f0_semitones_no_nan)  # Standard deviation\n",
    "    cv_smoothed_f0_semitones_no_nan = (std_smoothed_f0_semitones_no_nan / mean_smoothed_f0_semitones_no_nan) * 100  # Coefficient of Variation in percentage\n",
    "    training_features.loc[i, \"Pitch - Coefficient of Variation\"] = cv_smoothed_f0_semitones_no_nan\n",
    "\n",
    "    # Calculate the 20th, 50th, and 80th percentiles - pitch functional #3, 4, and 5\n",
    "    percentile_20_smoothed_f0_semitones_no_nan = np.percentile(smoothed_f0_semitones_no_nan, 20)\n",
    "    training_features.loc[i, \"Pitch - 20th Percentile\"] = percentile_20_smoothed_f0_semitones_no_nan\n",
    "    percentile_50_smoothed_f0_semitones_no_nan = np.percentile(smoothed_f0_semitones_no_nan, 50)\n",
    "    training_features.loc[i, \"Pitch - 50th Percentile\"] = percentile_50_smoothed_f0_semitones_no_nan\n",
    "    percentile_80_smoothed_f0_semitones_no_nan = np.percentile(smoothed_f0_semitones_no_nan, 80)\n",
    "    training_features.loc[i, \"Pitch - 80th Percentile\"] = percentile_80_smoothed_f0_semitones_no_nan\n",
    "\n",
    "    # Calculate the range between the 20th and 80th percentiles - pitch functional #6\n",
    "    percentile_range_smoothed_f0_semitones_no_nan = percentile_80_smoothed_f0_semitones_no_nan - percentile_20_smoothed_f0_semitones_no_nan\n",
    "    training_features.loc[i, \"Pitch - Range between 20th and 80th Percentile\"] = percentile_range_smoothed_f0_semitones_no_nan\n",
    "\n",
    "    # Calculate the slope of the signal (difference between consecutive values)\n",
    "    slopes = np.diff(smoothed_f0_semitones_no_nan)\n",
    "    \n",
    "    # Separate rising and falling slopes\n",
    "    rising_slopes = slopes[slopes > 0]    # Only positive slopes (rising parts)\n",
    "    falling_slopes = slopes[slopes < 0]   # Only negative slopes (falling parts)\n",
    "    \n",
    "    # Calculate mean and standard deviation for rising parts - pitch functional #7 and 8\n",
    "    mean_rising_slope_smoothed_f0_semitones_no_nan = np.mean(rising_slopes) if len(rising_slopes) > 0 else 0\n",
    "    training_features.loc[i, \"Pitch - Mean Rising Part\"] = mean_rising_slope_smoothed_f0_semitones_no_nan\n",
    "    std_rising_slope_smoothed_f0_semitones_no_nan = np.std(rising_slopes) if len(rising_slopes) > 0 else 0\n",
    "    training_features.loc[i, \"Pitch - Standard Deviation Rising Part\"] = std_rising_slope_smoothed_f0_semitones_no_nan\n",
    "    \n",
    "    # Calculate mean and standard deviation for falling parts - pitch functional #9 and 10\n",
    "    mean_falling_slope_smoothed_f0_semitones_no_nan = np.mean(falling_slopes) if len(falling_slopes) > 0 else 0\n",
    "    training_features.loc[i, \"Pitch - Mean Falling Part\"] = mean_falling_slope_smoothed_f0_semitones_no_nan\n",
    "    std_falling_slope_smoothed_f0_semitones_no_nan = np.std(falling_slopes) if len(falling_slopes) > 0 else 0\n",
    "    training_features.loc[i, \"Pitch - Standard Deviation Falling Part\"] = std_falling_slope_smoothed_f0_semitones_no_nan\n",
    "\n",
    "\n",
    "    \n",
    "    # Jitter extraction\n",
    "    # Remove unvoiced frames (f0 is NaN for unvoiced segments)\n",
    "    f0_jitter_no_nan = f0[voiced_flag]  # only consider voiced frames\n",
    "    f0_jitter_no_nan = np.nan_to_num(f0_jitter_no_nan, nan=0.0)  # Replace any remaining NaNs with 0 for safety\n",
    "    \n",
    "    # Calculate pitch periods (T0) in seconds\n",
    "    T0 = 1 / f0_jitter_no_nan\n",
    "    T0[T0 == np.inf] = 0  # Handle any inf values resulting from division by zero\n",
    "    \n",
    "    # Smooth T0 with a symmetric moving average filter (3 frames long)\n",
    "    window = np.ones(3) / 3  # 3-frame moving average filter\n",
    "    smoothed_T0 = convolve(T0, window, mode='same')\n",
    "    \n",
    "    # Convert smoothed_T0 to a PyTorch tensor for further processing\n",
    "    smoothed_T0_tensor = torch.tensor(smoothed_T0, dtype=torch.float32)\n",
    "    \n",
    "    # Calculate jitter as the absolute difference between consecutive T0 values\n",
    "    jitter_values = torch.abs(smoothed_T0_tensor[1:] - smoothed_T0_tensor[:-1])\n",
    "    \n",
    "    # Calculate arithmetic mean of jitter - jitter functional #1\n",
    "    mean_jitter = torch.mean(jitter_values)\n",
    "    training_features.loc[i, \"Jitter - Mean\"] = mean_jitter.item()\n",
    "    \n",
    "    \n",
    "    # Calculate coefficient of variation of jitter - jitter functional #2\n",
    "    std_jitter = torch.std(jitter_values)\n",
    "    cv_jitter = (std_jitter / mean_jitter) * 100  # Coefficient of Variation as percentage\n",
    "    training_features.loc[i, \"Jitter - Coefficient of Variation\"] = cv_jitter.item()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Mel-Frequency Cepstral Coefficients (MFCC) 1â€“4 extraction\n",
    "    # Convert the waveform to numpy (librosa works with numpy arrays)\n",
    "    waveform_np = waveform.numpy()\n",
    "    \n",
    "    # Extract MFCCs using librosa\n",
    "    mfccs = librosa.feature.mfcc(y = waveform_np, sr = sample_rate, n_mfcc=4)  # Extract only the first 4 MFCCs\n",
    "    \n",
    "    # Convert the MFCCs to a PyTorch tensor for further processing\n",
    "    mfccs_tensor = torch.tensor(mfccs, dtype=torch.float32)\n",
    "    \n",
    "    # Calculate arithmetic mean for each MFCC - MFCC functional #1, 2, 3, and 4\n",
    "    mean_mfcc1 = torch.mean(mfccs_tensor[0, 0, :])\n",
    "    training_features.loc[i, \"MFCC - Mean MFCC1\"] = mean_mfcc1.item()\n",
    "    mean_mfcc2 = torch.mean(mfccs_tensor[0, 1, :])\n",
    "    training_features.loc[i, \"MFCC - Mean MFCC2\"] = mean_mfcc2.item()\n",
    "    mean_mfcc3 = torch.mean(mfccs_tensor[0, 2, :])\n",
    "    training_features.loc[i, \"MFCC - Mean MFCC3\"] = mean_mfcc3.item()\n",
    "    mean_mfcc4 = torch.mean(mfccs_tensor[0, 3, :])\n",
    "    training_features.loc[i, \"MFCC - Mean MFCC4\"] = mean_mfcc4.item()\n",
    "\n",
    "    # Calculate coefficient of variation for each MFCC - MFCC functional #5, 6, 7, and 8\n",
    "    std_mfcc1 = torch.std(mfccs_tensor[0, 0, :])\n",
    "    std_mfcc2 = torch.std(mfccs_tensor[0, 1, :])\n",
    "    std_mfcc3 = torch.std(mfccs_tensor[0, 2, :])\n",
    "    std_mfcc4 = torch.std(mfccs_tensor[0, 3, :])\n",
    "    \n",
    "    cv_mfcc1 = (std_mfcc1 / mean_mfcc1) * 100  # Coefficient of Variation in percentage\n",
    "    training_features.loc[i, \"MFCC - Coefficient of Variation MFCC1\"] = cv_mfcc1.item()\n",
    "    cv_mfcc2 = (std_mfcc2 / mean_mfcc2) * 100\n",
    "    training_features.loc[i, \"MFCC - Coefficient of Variation MFCC2\"] = cv_mfcc2.item()\n",
    "    cv_mfcc3 = (std_mfcc3 / mean_mfcc3) * 100\n",
    "    training_features.loc[i, \"MFCC - Coefficient of Variation MFCC3\"] = cv_mfcc3.item()\n",
    "    cv_mfcc4 = (std_mfcc4 / mean_mfcc4) * 100\n",
    "    training_features.loc[i, \"MFCC - Coefficient of Variation MFCC4\"] = cv_mfcc4.item()\n",
    "\n",
    "    # Calculate arithmetic mean for each MFCC only in the voiced region - MFCC functional #9, 10, 11, and 12\n",
    "    mean_mfcc1_voiced = torch.mean(mfccs_tensor[0, 0, voiced_flag[0]])\n",
    "    training_features.loc[i, \"MFCC - Mean MFCC1 Voiced Region\"] = mean_mfcc1_voiced.item()\n",
    "    mean_mfcc2_voiced = torch.mean(mfccs_tensor[0, 1, voiced_flag[0]])\n",
    "    training_features.loc[i, \"MFCC - Mean MFCC2 Voiced Region\"] = mean_mfcc2_voiced.item()\n",
    "    mean_mfcc3_voiced = torch.mean(mfccs_tensor[0, 2, voiced_flag[0]])\n",
    "    training_features.loc[i, \"MFCC - Mean MFCC3 Voiced Region\"] = mean_mfcc3_voiced.item()\n",
    "    mean_mfcc4_voiced = torch.mean(mfccs_tensor[0, 3, voiced_flag[0]])\n",
    "    training_features.loc[i, \"MFCC - Mean MFCC4 Voiced Region\"] = mean_mfcc4_voiced.item()\n",
    "\n",
    "    # Calculate coefficient of variation for each MFCC only in the voiced region - MFCC functional #13, 14, 15, and 16\n",
    "    std_mfcc1_voiced = torch.std(mfccs_tensor[0, 0, voiced_flag[0]])\n",
    "    std_mfcc2_voiced = torch.std(mfccs_tensor[0, 1, voiced_flag[0]])\n",
    "    std_mfcc3_voiced = torch.std(mfccs_tensor[0, 2, voiced_flag[0]])\n",
    "    std_mfcc4_voiced = torch.std(mfccs_tensor[0, 3, voiced_flag[0]])\n",
    "    \n",
    "    cv_mfcc1_voiced = (std_mfcc1_voiced / mean_mfcc1_voiced) * 100  # Coefficient of Variation in percentage\n",
    "    training_features.loc[i, \"MFCC - Coefficient of Variation MFCC1 Voiced Region\"] = cv_mfcc1_voiced.item()\n",
    "    cv_mfcc2_voiced = (std_mfcc2_voiced / mean_mfcc2_voiced) * 100\n",
    "    training_features.loc[i, \"MFCC - Coefficient of Variation MFCC2 Voiced Region\"] = cv_mfcc2_voiced.item()\n",
    "    cv_mfcc3_voiced = (std_mfcc3_voiced / mean_mfcc3_voiced) * 100\n",
    "    training_features.loc[i, \"MFCC - Coefficient of Variation MFCC3 Voiced Region\"] = cv_mfcc3_voiced.item()\n",
    "    cv_mfcc4_voiced = (std_mfcc4_voiced / mean_mfcc4_voiced) * 100\n",
    "    training_features.loc[i, \"MFCC - Coefficient of Variation MFCC4 Voiced Region\"] = cv_mfcc4_voiced.item()\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(0, len(testing)):\n",
    "for i in range(0, 10):\n",
    "       \n",
    "    # Load in the audio file\n",
    "    path = testing[\"Filepath\"].iloc[i]\n",
    "    waveform, sample_rate = torchaudio.load(path)\n",
    "\n",
    "    \n",
    "\n",
    "    # Pitch extraction\n",
    "    # Convert the waveform to numpy (librosa works with numpy arrays)\n",
    "    waveform_np = waveform.numpy()\n",
    "\n",
    "    # Use librosa to extract the pitch\n",
    "    # By default, the window size is usually 2048 samples, and the hop length (the distance between consecutive windows) is typically 512 samples.\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(waveform_np, fmin=librosa.note_to_hz('A0'), fmax=librosa.note_to_hz('C8'))\n",
    "    \n",
    "    # Convert the pitch (f0) to a semitone scale starting at 27.5 Hz (A0)\n",
    "    f0_semitones = 12 * np.log2(f0 / 27.5)\n",
    "\n",
    "    # Remove NaN values (no pitch detected)\n",
    "    f0_semitones_no_nan = f0_semitones[voiced_flag]  # only consider voiced frames\n",
    "\n",
    "    # Apply symmetric moving average filter (3 frames long)\n",
    "    # The window size is 3, so the filter is [1/3, 1/3, 1/3] for a symmetric average.\n",
    "    window = np.ones(3) / 3  # symmetric moving average filter\n",
    "    smoothed_f0_semitones_no_nan = convolve(f0_semitones_no_nan, window, mode='same')\n",
    "\n",
    "    # Arithmetic Mean (AM) - pitch funtional #1\n",
    "    mean_smoothed_f0_semitones_no_nan = np.mean(smoothed_f0_semitones_no_nan)\n",
    "    testing_features.loc[i, \"Pitch - Mean\"] = mean_smoothed_f0_semitones_no_nan\n",
    "\n",
    "    # Coefficient of Variation (CV) - pitch functional #2\n",
    "    std_smoothed_f0_semitones_no_nan = np.std(smoothed_f0_semitones_no_nan)  # Standard deviation\n",
    "    cv_smoothed_f0_semitones_no_nan = (std_smoothed_f0_semitones_no_nan / mean_smoothed_f0_semitones_no_nan) * 100  # Coefficient of Variation in percentage\n",
    "    testing_features.loc[i, \"Pitch - Coefficient of Variation\"] = cv_smoothed_f0_semitones_no_nan\n",
    "\n",
    "    # Calculate the 20th, 50th, and 80th percentiles - pitch functional #3, 4, and 5\n",
    "    percentile_20_smoothed_f0_semitones_no_nan = np.percentile(smoothed_f0_semitones_no_nan, 20)\n",
    "    testing_features.loc[i, \"Pitch - 20th Percentile\"] = percentile_20_smoothed_f0_semitones_no_nan\n",
    "    percentile_50_smoothed_f0_semitones_no_nan = np.percentile(smoothed_f0_semitones_no_nan, 50)\n",
    "    testing_features.loc[i, \"Pitch - 50th Percentile\"] = percentile_50_smoothed_f0_semitones_no_nan\n",
    "    percentile_80_smoothed_f0_semitones_no_nan = np.percentile(smoothed_f0_semitones_no_nan, 80)\n",
    "    testing_features.loc[i, \"Pitch - 80th Percentile\"] = percentile_80_smoothed_f0_semitones_no_nan\n",
    "\n",
    "    # Calculate the range between the 20th and 80th percentiles - pitch functional #6\n",
    "    percentile_range_smoothed_f0_semitones_no_nan = percentile_80_smoothed_f0_semitones_no_nan - percentile_20_smoothed_f0_semitones_no_nan\n",
    "    testing_features.loc[i, \"Pitch - Range between 20th and 80th Percentile\"] = percentile_range_smoothed_f0_semitones_no_nan\n",
    "\n",
    "    # Calculate the slope of the signal (difference between consecutive values)\n",
    "    slopes = np.diff(smoothed_f0_semitones_no_nan)\n",
    "    \n",
    "    # Separate rising and falling slopes\n",
    "    rising_slopes = slopes[slopes > 0]    # Only positive slopes (rising parts)\n",
    "    falling_slopes = slopes[slopes < 0]   # Only negative slopes (falling parts)\n",
    "    \n",
    "    # Calculate mean and standard deviation for rising parts - pitch functional #7 and 8\n",
    "    mean_rising_slope_smoothed_f0_semitones_no_nan = np.mean(rising_slopes) if len(rising_slopes) > 0 else 0\n",
    "    testing_features.loc[i, \"Pitch - Mean Rising Part\"] = mean_rising_slope_smoothed_f0_semitones_no_nan\n",
    "    std_rising_slope_smoothed_f0_semitones_no_nan = np.std(rising_slopes) if len(rising_slopes) > 0 else 0\n",
    "    testing_features.loc[i, \"Pitch - Standard Deviation Rising Part\"] = std_rising_slope_smoothed_f0_semitones_no_nan\n",
    "    \n",
    "    # Calculate mean and standard deviation for falling parts - pitch functional #9 and 10\n",
    "    mean_falling_slope_smoothed_f0_semitones_no_nan = np.mean(falling_slopes) if len(falling_slopes) > 0 else 0\n",
    "    testing_features.loc[i, \"Pitch - Mean Falling Part\"] = mean_falling_slope_smoothed_f0_semitones_no_nan\n",
    "    std_falling_slope_smoothed_f0_semitones_no_nan = np.std(falling_slopes) if len(falling_slopes) > 0 else 0\n",
    "    testing_features.loc[i, \"Pitch - Standard Deviation Falling Part\"] = std_falling_slope_smoothed_f0_semitones_no_nan\n",
    "\n",
    "\n",
    "    \n",
    "    # Jitter extraction\n",
    "    # Remove unvoiced frames (f0 is NaN for unvoiced segments)\n",
    "    f0_jitter_no_nan = f0[voiced_flag]  # only consider voiced frames\n",
    "    f0_jitter_no_nan = np.nan_to_num(f0_jitter_no_nan, nan=0.0)  # Replace any remaining NaNs with 0 for safety\n",
    "    \n",
    "    # Calculate pitch periods (T0) in seconds\n",
    "    T0 = 1 / f0_jitter_no_nan\n",
    "    T0[T0 == np.inf] = 0  # Handle any inf values resulting from division by zero\n",
    "    \n",
    "    # Smooth T0 with a symmetric moving average filter (3 frames long)\n",
    "    window = np.ones(3) / 3  # 3-frame moving average filter\n",
    "    smoothed_T0 = convolve(T0, window, mode='same')\n",
    "    \n",
    "    # Convert smoothed_T0 to a PyTorch tensor for further processing\n",
    "    smoothed_T0_tensor = torch.tensor(smoothed_T0, dtype=torch.float32)\n",
    "    \n",
    "    # Calculate jitter as the absolute difference between consecutive T0 values\n",
    "    jitter_values = torch.abs(smoothed_T0_tensor[1:] - smoothed_T0_tensor[:-1])\n",
    "    \n",
    "    # Calculate arithmetic mean of jitter - jitter functional #1\n",
    "    mean_jitter = torch.mean(jitter_values)\n",
    "    testing_features.loc[i, \"Jitter - Mean\"] = mean_jitter.item()\n",
    "    \n",
    "    \n",
    "    # Calculate coefficient of variation of jitter - jitter functional #2\n",
    "    std_jitter = torch.std(jitter_values)\n",
    "    cv_jitter = (std_jitter / mean_jitter) * 100  # Coefficient of Variation as percentage\n",
    "    testing_features.loc[i, \"Jitter - Coefficient of Variation\"] = cv_jitter.item()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Mel-Frequency Cepstral Coefficients (MFCC) 1â€“4 extraction\n",
    "    # Convert the waveform to numpy (librosa works with numpy arrays)\n",
    "    waveform_np = waveform.numpy()\n",
    "    \n",
    "    # Extract MFCCs using librosa\n",
    "    mfccs = librosa.feature.mfcc(y = waveform_np, sr = sample_rate, n_mfcc=4)  # Extract only the first 4 MFCCs\n",
    "    \n",
    "    # Convert the MFCCs to a PyTorch tensor for further processing\n",
    "    mfccs_tensor = torch.tensor(mfccs, dtype=torch.float32)\n",
    "    \n",
    "    # Calculate arithmetic mean for each MFCC - MFCC functional #1, 2, 3, and 4\n",
    "    mean_mfcc1 = torch.mean(mfccs_tensor[0, 0, :])\n",
    "    testing_features.loc[i, \"MFCC - Mean MFCC1\"] = mean_mfcc1.item()\n",
    "    mean_mfcc2 = torch.mean(mfccs_tensor[0, 1, :])\n",
    "    testing_features.loc[i, \"MFCC - Mean MFCC2\"] = mean_mfcc2.item()\n",
    "    mean_mfcc3 = torch.mean(mfccs_tensor[0, 2, :])\n",
    "    testing_features.loc[i, \"MFCC - Mean MFCC3\"] = mean_mfcc3.item()\n",
    "    mean_mfcc4 = torch.mean(mfccs_tensor[0, 3, :])\n",
    "    testing_features.loc[i, \"MFCC - Mean MFCC4\"] = mean_mfcc4.item()\n",
    "\n",
    "    # Calculate coefficient of variation for each MFCC - MFCC functional #5, 6, 7, and 8\n",
    "    std_mfcc1 = torch.std(mfccs_tensor[0, 0, :])\n",
    "    std_mfcc2 = torch.std(mfccs_tensor[0, 1, :])\n",
    "    std_mfcc3 = torch.std(mfccs_tensor[0, 2, :])\n",
    "    std_mfcc4 = torch.std(mfccs_tensor[0, 3, :])\n",
    "    \n",
    "    cv_mfcc1 = (std_mfcc1 / mean_mfcc1) * 100  # Coefficient of Variation in percentage\n",
    "    testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC1\"] = cv_mfcc1.item()\n",
    "    cv_mfcc2 = (std_mfcc2 / mean_mfcc2) * 100\n",
    "    testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC2\"] = cv_mfcc2.item()\n",
    "    cv_mfcc3 = (std_mfcc3 / mean_mfcc3) * 100\n",
    "    testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC3\"] = cv_mfcc3.item()\n",
    "    cv_mfcc4 = (std_mfcc4 / mean_mfcc4) * 100\n",
    "    testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC4\"] = cv_mfcc4.item()\n",
    "\n",
    "    # Calculate arithmetic mean for each MFCC only in the voiced region - MFCC functional #9, 10, 11, and 12\n",
    "    mean_mfcc1_voiced = torch.mean(mfccs_tensor[0, 0, voiced_flag[0]])\n",
    "    testing_features.loc[i, \"MFCC - Mean MFCC1 Voiced Region\"] = mean_mfcc1_voiced.item()\n",
    "    mean_mfcc2_voiced = torch.mean(mfccs_tensor[0, 1, voiced_flag[0]])\n",
    "    testing_features.loc[i, \"MFCC - Mean MFCC2 Voiced Region\"] = mean_mfcc2_voiced.item()\n",
    "    mean_mfcc3_voiced = torch.mean(mfccs_tensor[0, 2, voiced_flag[0]])\n",
    "    testing_features.loc[i, \"MFCC - Mean MFCC3 Voiced Region\"] = mean_mfcc3_voiced.item()\n",
    "    mean_mfcc4_voiced = torch.mean(mfccs_tensor[0, 3, voiced_flag[0]])\n",
    "    testing_features.loc[i, \"MFCC - Mean MFCC4 Voiced Region\"] = mean_mfcc4_voiced.item()\n",
    "\n",
    "    # Calculate coefficient of variation for each MFCC only in the voiced region - MFCC functional #13, 14, 15, and 16\n",
    "    std_mfcc1_voiced = torch.std(mfccs_tensor[0, 0, voiced_flag[0]])\n",
    "    std_mfcc2_voiced = torch.std(mfccs_tensor[0, 1, voiced_flag[0]])\n",
    "    std_mfcc3_voiced = torch.std(mfccs_tensor[0, 2, voiced_flag[0]])\n",
    "    std_mfcc4_voiced = torch.std(mfccs_tensor[0, 3, voiced_flag[0]])\n",
    "    \n",
    "    cv_mfcc1_voiced = (std_mfcc1_voiced / mean_mfcc1_voiced) * 100  # Coefficient of Variation in percentage\n",
    "    testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC1 Voiced Region\"] = cv_mfcc1_voiced.item()\n",
    "    cv_mfcc2_voiced = (std_mfcc2_voiced / mean_mfcc2_voiced) * 100\n",
    "    testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC2 Voiced Region\"] = cv_mfcc2_voiced.item()\n",
    "    cv_mfcc3_voiced = (std_mfcc3_voiced / mean_mfcc3_voiced) * 100\n",
    "    testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC3 Voiced Region\"] = cv_mfcc3_voiced.item()\n",
    "    cv_mfcc4_voiced = (std_mfcc4_voiced / mean_mfcc4_voiced) * 100\n",
    "    testing_features.loc[i, \"MFCC - Coefficient of Variation MFCC4 Voiced Region\"] = cv_mfcc4_voiced.item()\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate features to the original dataframes\n",
    "training_with_Features = pd.concat([training, training_features], axis=1)\n",
    "testing_with_Features = pd.concat([testing, testing_features], axis=1)\n",
    "\n",
    "# Save the updated dataframes as .csv files\n",
    "training_with_Features.to_csv('/kaggle/working/training_with_Features.csv', index=False)\n",
    "testing_with_Features.to_csv('/kaggle/working/testing_with_Features.csv', index=False)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8278e1d1",
   "metadata": {
    "papermill": {
     "duration": 0.011559,
     "end_time": "2024-11-11T06:33:12.470288",
     "exception": false,
     "start_time": "2024-11-11T06:33:12.458729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdcccaca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:33:12.496275Z",
     "iopub.status.busy": "2024-11-11T06:33:12.495363Z",
     "iopub.status.idle": "2024-11-11T06:33:12.499484Z",
     "shell.execute_reply": "2024-11-11T06:33:12.498786Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.018992,
     "end_time": "2024-11-11T06:33:12.501345",
     "exception": false,
     "start_time": "2024-11-11T06:33:12.482353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mode (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db9b529f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T06:33:12.526898Z",
     "iopub.status.busy": "2024-11-11T06:33:12.526151Z",
     "iopub.status.idle": "2024-11-11T06:33:12.551263Z",
     "shell.execute_reply": "2024-11-11T06:33:12.550435Z"
    },
    "papermill": {
     "duration": 0.040069,
     "end_time": "2024-11-11T06:33:12.553354",
     "exception": false,
     "start_time": "2024-11-11T06:33:12.513285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019635677337646484\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "path = '/kaggle/input/metadata-and-augmentations/augmented_training_samples/03-01-01-01-01-01-01_augmented_training_V1_322.wav'\n",
    "torchaudio.load(path)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad0c12",
   "metadata": {
    "papermill": {
     "duration": 0.011728,
     "end_time": "2024-11-11T06:33:12.577230",
     "exception": false,
     "start_time": "2024-11-11T06:33:12.565502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 107620,
     "sourceId": 256618,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 316368,
     "sourceId": 639622,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 325566,
     "sourceId": 653195,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 338555,
     "sourceId": 671851,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1118008,
     "sourceId": 1877714,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5928199,
     "sourceId": 9695632,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6034593,
     "sourceId": 9862609,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 104.72445,
   "end_time": "2024-11-11T06:33:15.207123",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-11T06:31:30.482673",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
